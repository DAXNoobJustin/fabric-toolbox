{"cells":[{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50b46991-79b4-43ed-a3c1-45caa9767963"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b7e81de0-32ed-4552-b145-399179df0802"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6720a804-bf22-4e4e-b036-2b55ca989c01"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'SourceWorkspaceName','TargetWorkspaceName','Item', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10231433-2527-4504-8e95-22b1933657e2"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e871a7f2-f46f-4d25-9f5e-4b482692db38"},{"cell_type":"markdown","source":["pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pDebugMode = \"yes\""],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"d53db2fe-ece6-4d3b-9186-125a40e9fcd7"},{"cell_type":"markdown","source":["**Resolve the source and target workspaces**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3e6b114-71a6-4a1a-b249-2beddde1b0c8"},{"cell_type":"code","source":["vSourceWorkspaceName = fabric.resolve_workspace_name(pSourceWorkspaceId)\n","vTargetWorkspaceName = fabric.resolve_workspace_name(pTargetWorkspaceId)\n","vSourceWorkspaceId = pSourceWorkspaceId\n","vTargetWorkspaceId = pTargetWorkspaceId"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"26ccafd9-5ce4-42d8-9769-d5417801ffc4"},{"cell_type":"markdown","source":["**List of notebooks in source workspace --> semantic link labs have no function as of 22.02.2025**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be1db7ae-d044-43f7-b80b-d60094dd7517"},{"cell_type":"code","source":["df_source_items = fabric.list_items(workspace=vSourceWorkspaceName)\n","df_source_notebooks = df_source_items[df_source_items['Type']=='Notebook']"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a2cd073-cafc-47e3-9ee9-c47cbb8274cc"},{"cell_type":"markdown","source":["**Verify that there is a least one notebook in the source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"70d5da33-3dcc-403e-ae0c-23836edcfeb9"},{"cell_type":"code","source":["if df_source_notebooks.empty:\n","    vMessage = f\"workspace <vSourceWorkspaceName> have 0 notebook. post-update is not required.\"\n","\n","    # Display an exit message\n","    display(Markdown(\"### âœ… Notebook execution stopped successfully!\"))\n","\n","    # Exit without error\n","    mssparkutils.notebook.exit(vMessage)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ff555d58-ae4d-4134-ad91-8753303f9878"},{"cell_type":"markdown","source":["**Update notebook dependencies**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d82934ea-5272-43be-9087-8f976eac8b99"},{"cell_type":"code","source":["# get the list of data pipelines in the target workspace\n","df_notebooks = notebookutils.notebook.list(workspaceId=vTargetWorkspaceId)\n","# df_notebooks\n","\n","for notebook in df_notebooks:\n","\n","    # get the notebook id and display name\n","    vNotebookId = notebook.id\n","    vNotebookName = notebook.displayName\n","   \n","\n","    # get the current notebook definition\n","    vNotebookDefinition = notebookutils.notebook.getDefinition(name=vNotebookName, workspaceId=vSourceWorkspaceId) \n","    vNotebookJson = json.loads(vNotebookDefinition)\n","\n","    # update lakehouse dependencies\n","    try:\n","\n","        # check and remove any attached lakehouses\n","        if 'dependencies' in vNotebookJson['metadata'] \\\n","            and 'lakehouse' in vNotebookJson['metadata']['dependencies'] \\\n","            and vNotebookJson['metadata'][\"dependencies\"][\"lakehouse\"] is not None:\n","\n","            vCurrentLakehouse = vNotebookJson['metadata']['dependencies']['lakehouse']\n","\n","            if 'default_lakehouse_name' in vCurrentLakehouse:\n","\n","                vNotebookJson['metadata']['dependencies']['lakehouse'] = {}\n","                print(f\"attempting to update notebook <{vNotebookName}> with new default lakehouse: {vCurrentLakehouse['default_lakehouse_name']} in workspace <{vTargetWorkspaceName}>.\")\n","\n","                # update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","                notebookutils.notebook.updateDefinition(\n","                    name = vNotebookName,\n","                    content  = json.dumps(vNotebookJson),  \n","                    defaultLakehouse = vCurrentLakehouse['default_lakehouse_name'],\n","                    defaultLakehouseWorkspace = vTargetWorkspaceId,\n","                    workspaceId = vTargetWorkspaceId\n","                )\n","\n","                print(f\"updated notebook <{vNotebookName}> in workspace <{vTargetWorkspaceName}>.\")\n","\n","            else:\n","                print(f'no default lakehouse set for notebook <{vNotebookName}>, ignoring.')\n","\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vNotebookName, 'update lakehouse dependencies', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vNotebookName, 'update lakehouse dependencies', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n","    # update warehouse dependencies\n","    try:\n","        if 'dependencies' in vNotebookJson['metadata'] and 'warehouse' in vNotebookJson['metadata']['dependencies']:\n","            \n","            #fetch existing details\n","            vCurrentWarehouse = vNotebookJson['metadata']['dependencies']['warehouse']\n","            vCurrentWarehouseId = vCurrentWarehouse['default_warehouse']\n","            vCurrentWarehouseName =  fabric.resolve_item_name(item_id = vCurrentWarehouseId, workspace=vSourceWorkspaceId)\n","            vTargetWarehouseId = fabric.resolve_item_id(item_name = vCurrentWarehouseName, type='Warehouse', workspace=vTargetWorkspaceId)\n","\n","            if 'default_warehouse' in vCurrentWarehouse:\n","\n","                print(f\"attempting to update notebook {vNotebookName} with new default warehouse: {vTargetWarehouseId} in {vTargetWorkspaceName}\")\n","            \n","                # update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","                vNotebookJson['metadata']['dependencies']['warehouse']['default_warehouse'] = vTargetWarehouseId\n","                for warehouse in vNotebookJson['metadata']['dependencies']['warehouse']['known_warehouses']:\n","                    if warehouse['id'] == vCurrentWarehouseId:\n","                        warehouse['id'] = vTargetWarehouseId\n","                # print(json.dumps(vNotebookJson, indent=4))\n","                notebookutils.notebook.updateDefinition(\n","                    name = vNotebookName,\n","                    content  = json.dumps(vNotebookJson),\n","                    workspaceId = vTargetWorkspaceId\n","                )\n","                print(f\"updated notebook {vNotebookName} in {vTargetWorkspaceName}\")\n","\n","            else:\n","                print(f\"no default warehouse was found in the source notebook {vNotebookName} there cannot set default for target\")\n","\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vNotebookName, 'update warehouse dependencies', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vNotebookName, 'update warehouse dependencies', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7c850199-4380-4369-8dba-a6631136d052"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"82f8bea3-217b-490a-8fb3-8cd3eacd6fba"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"SourceWorkspaceName\" : \"string\",\n","            \"TargetWorkspaceName\" : \"string\",\n","            \"Item\":\"string\",\n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging_cicd\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d6d6709-0ac8-482a-a8ef-be2ddcb4ce2a"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"widgets":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}