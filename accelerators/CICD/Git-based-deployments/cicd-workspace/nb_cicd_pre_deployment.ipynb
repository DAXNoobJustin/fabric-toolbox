{"cells":[{"cell_type":"markdown","source":["**Libraries**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2ee3b3a-f369-4f98-8e88-a04e999870cf"},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime, timedelta\n","import sempy.fabric as fabric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"151a5189-710d-4f0a-ad72-5ad9c422c817"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4788f8ff-57e3-44dc-82d2-225b1f99ba38"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"121fc594-15d2-47d2-b4d0-f9609f50791b"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8421d0a-f588-4f23-bd13-4696e6f2fcc5"},{"cell_type":"markdown","source":["pToken = \"\"\n","pSqlToken = \"\"\n","pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pDebugMode = \"yes\"\n","pFeatureBranch = \"\"\n","pOnelakeRoles = ''\n","pOnelakeRules = ''\n","pOnelakeEntraMembers = ''\n","pOnelakeItemMembers = ''"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"22da0c79-bfea-43ea-9d57-583771c4f22f"},{"cell_type":"markdown","source":["**Access token**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b3eda308-20f6-4ff1-87a3-5c71b228f3a1"},{"cell_type":"code","source":["vScope = \"https://analysis.windows.net/powerbi/api\"\n","\n","# get the access token \n","if pDebugMode == \"yes\":\n","    # in debug mode, use the token of the current user\n","    vAccessToken  = mssparkutils.credentials.getToken(vScope)\n","    vSqlAccessToken = vAccessToken\n","else:\n","    # when the code is run from the pipelines, to token is generated in a previous step and passed as a parameter to the notebook\n","    vAccessToken = pToken \n","    vSqlAccessToken = pSqlToken"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f3095581-b39f-40a9-8523-6800e182f4e9"},{"cell_type":"markdown","source":["**Check if the source workspace passed from DevOps equals the feature branch name**\n","- This is a specific handling when a PR is done from the feature branch"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cc40df1-4833-4b4a-8f16-a4c7399d0a0d"},{"cell_type":"code","source":["if pSourceWorkspaceId == pFeatureBranch:\n","    pSourceWorkspaceId = fabric.resolve_workspace_id(workspace=pFeatureBranch)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7a0a085c-fa84-4fe1-af36-b089366d95a9"},{"cell_type":"markdown","source":["**Define the DAG**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a8b6198-1624-4008-9f6d-a4133b4650ab"},{"cell_type":"code","source":["dagList = []\n","\n","# add to the DAG list nb_cicd_pre_update_lakehouses\n","dagList.append({\n","            \"name\": \"nb_cicd_pre_update_lakehouses\",\n","            \"path\": \"nb_cicd_pre_update_lakehouses\",\n","            \"timeoutPerCellInSeconds\": 300,\n","            \"args\": {\n","                \"useRootDefaultLakehouse\": True,\n","                \"pToken\": vAccessToken,\n","                \"pSqlToken\": vSqlAccessToken,\n","                \"pSourceWorkspaceId\":pSourceWorkspaceId,\n","                \"pTargetWorkspaceId\":pTargetWorkspaceId,\n","                \"pDebugMode\":pDebugMode,\n","                \"pOnelakeRoles\":pOnelakeRoles,\n","                \"pOnelakeRules\":pOnelakeRules,\n","                \"pOnelakeEntraMembers\":pOnelakeEntraMembers,\n","                \"pOnelakeItemMembers\":pOnelakeItemMembers,\n","                }\n","        })\n","\n","# add to the DAG list nb_cicd_pre_update_warehouses\n","dagList.append({\n","            \"name\": \"nb_cicd_pre_update_warehouses\",\n","            \"path\": \"nb_cicd_pre_update_warehouses\",\n","            \"timeoutPerCellInSeconds\": 300,\n","            \"args\": {\n","                \"useRootDefaultLakehouse\": True,\n","                \"pSqlToken\": vSqlAccessToken,\n","                \"pSourceWorkspaceId\":pSourceWorkspaceId,\n","                \"pTargetWorkspaceId\":pTargetWorkspaceId,\n","                \"pDebugMode\":pDebugMode\n","                },\n","            \"dependencies\": [\"nb_cicd_pre_update_lakehouses\"]\n","        })\n","\n","DAG = { \"activities\": dagList,\"concurrency\": 2, \"timeoutInSeconds\": 900 }\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb40613c-1d2b-4edf-8d1e-de25de315fec"},{"cell_type":"markdown","source":["**Run multiple**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3345dfe0-84c7-4c33-9742-b726e28ef82e"},{"cell_type":"code","source":["try:\n","    mssparkutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\": True})\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'running the DAG', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'running the DAG', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21fdc174-a457-4bf3-9b38-5fb7c8d8a68d"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89352377-24c6-4f57-9ba1-18532e7587ba"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2daf311c-d307-4ddd-83eb-0d7853214cc8"},{"cell_type":"markdown","source":["**Exit notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4fcc01a8-abab-420b-b7ae-d72208eabcfd"},{"cell_type":"code","source":["mssparkutils.notebook.exit(f\"Notebook <{vLogNotebookName}> run successfully. Check logging table in CI/CD lakehouse for more details.\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e97c7158-a8fa-4913-84a0-024246cf9275"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"widgets":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}