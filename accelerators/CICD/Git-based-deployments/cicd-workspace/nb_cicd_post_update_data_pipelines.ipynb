{"cells":[{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2b41137c-c931-4ab3-8164-7011a881004a"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"45df1b7b-aa26-459d-a5fe-b8e435704198"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"37b323db-d161-430e-99f7-f4f538fad214"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'SourceWorkspaceName','TargetWorkspaceName','Item', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc585dc4-692b-4741-a978-b38f890273c3"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8737f968-0f8f-4547-b548-c63dfd99b7ca"},{"cell_type":"markdown","source":["pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pTargetStage = \"Stage2\"\n","pDebugMode = \"yes\"\n","pProjectName = \"fabric-cicd\"\n","pMappingConnections = ''"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"eed9f97f-a570-45cf-b270-334335d9adc7"},{"cell_type":"markdown","source":["**Resolve source and target workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"957716f1-81f3-4f78-8a73-a97f58788db0"},{"cell_type":"code","source":["vSourceWorkspaceName = fabric.resolve_workspace_name(pSourceWorkspaceId)\n","vTargetWorkspaceName = fabric.resolve_workspace_name(pTargetWorkspaceId)\n","vSourceWorkspaceId = pSourceWorkspaceId\n","vTargetWorkspaceId = pTargetWorkspaceId"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"67120adb-033f-48b1-9476-928593c6090d"},{"cell_type":"markdown","source":["**List of data pipelines in source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a912284e-2e7e-4f88-bfa3-87cf210e38c4"},{"cell_type":"code","source":["df_source_data_pipelines = labs.list_data_pipelines(workspace=vSourceWorkspaceName)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c0fc35f-73f8-450a-836c-cc29f4e033d5"},{"cell_type":"markdown","source":["**Verify that there is a least one data pipeline in the source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"71feb824-6baa-4336-a707-761ac07697a6"},{"cell_type":"code","source":["if df_source_data_pipelines.empty:\n","    vMessage = f\"workspace <vSourceWorkspaceName> have 0 data pipeline. post-update is not required.\"\n","\n","    # Display an exit message\n","    display(Markdown(\"### âœ… Notebook execution stopped successfully!\"))\n","\n","    # Exit without error\n","    mssparkutils.notebook.exit(vMessage)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f0fc4a5c-5a55-40d4-825a-8e5bd5e0554f"},{"cell_type":"markdown","source":["**Get the connections mapping between Stages and list existing fabric connections**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"75302767-6169-4f18-a3ff-41ee98a6f600"},{"cell_type":"code","source":["# get the mapping of connections between stages\n","mapping_connections_json = json.loads(pMappingConnections)\n","df_mapping_connections = pd.DataFrame(mapping_connections_json)\n","\n","# get the list of existing connections in the tenant. the list will be used for lookups \n","df_existing_connections = labs.list_connections()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"8df6f768-a06c-468f-8bd8-e5bb402f32fd"},{"cell_type":"markdown","source":["**Functions**\n","- validate_stage_connection_id\n","- find_connection_id\n","- update_pipeline_connections\n","- update_linked_services\n","- update_notebooks\n","- update_fabric_pipelines\n","- update_semantic_models\n","- update_data_pipeline_definition"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b402021a-292c-4e01-bc2d-751b1c5f6367"},{"cell_type":"code","source":["def validate_stage_connection_id(connectionId):\n","\n","    if connectionId in df_existing_connections['Connection Id'].values:\n","        vMessage = f\"connection id <{connectionId}> is valid>\"\n","        print(f\"{vMessage}\") \n","        vConnectionValidation = \"valid\"\n","    else:\n","        vMessage = f\"connection id <{connectionId}> is unvalid>\"\n","        print(f\"{vMessage}\") \n","        vConnectionValidation = \"unvalid\"\n","    return vConnectionValidation"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a57af11e-f747-41e8-94a4-6b7385ee3629"},{"cell_type":"code","source":["# function to find a connection id based on the target stage\n","# the csv provided with the mapping between stages is used, with the assomption of 4 stages maximun (dev, test, uat, prod)\n","def find_connection_id(devConnectionId, targetStage):\n","\n","    global df_mapping_connections\n","\n","    vMessage = f\"dev connection id is <{devConnectionId}>\"\n","    print(f\"{vMessage}\") \n","\n","    # filter the DataFrame based on a condition\n","    df_mapping_connections_filtered = df_mapping_connections[(df_mapping_connections['ConnectionStage1'] == devConnectionId)]\n","\n","    # extract the value of a target connection id\n","    # if the target connection cannot be found assign it the dev connection to avoid breaking the json definition of the pipeline\n","    if not df_mapping_connections_filtered.empty:\n","\n","        first_row = df_mapping_connections_filtered.iloc[0]  # Get the first matching row\n","\n","        if targetStage == \"Stage2\":\n","            targetConnectionId = first_row[\"ConnectionStage2\"]\n","\n","        elif targetStage == \"Stage3\":\n","            targetConnectionId = first_row[\"ConnectionStage3\"]\n","        else:\n","            targetConnectionId = first_row[\"ConnectionStage4\"]\n","\n","        # if the stage column in the mapping has no value, assing NA\n","        targetConnectionId = \"NA\" if pd.isna(targetConnectionId) or targetConnectionId == \"\" else targetConnectionId\n","\n","        # validate that the stage connection exists\n","        vConnectionValidation = validate_stage_connection_id(targetConnectionId)\n","\n","        # if the validation of the connection fails , keep the dev connection\n","        if vConnectionValidation == \"unvalid\":\n","            targetConnectionId = devConnectionId\n","\n","    else:\n","        \n","        vMessage = f\"no valid connection found in the mapping matching the condition, source connection will be kept\"\n","        print(f\"{vMessage}\") \n","\n","        # assign the dev connection to the target connection\n","        targetConnectionId = devConnectionId\n","\n","\n","    # return the found values\n","    return targetConnectionId"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e58d08ae-8d9b-450f-8e52-8a03323662dd"},{"cell_type":"code","source":["# function to parse the json of the pipeline and update connections\n","def update_pipeline_connections(obj, stage):\n","\n","    if isinstance(obj, dict):\n","        for key, value in obj.items():\n","            # if the key is a connection\n","            if key == \"connection\":\n","                \n","                # find the dev connection id (Stage1) \n","                devConnectionId = value\n","\n","                # lookup the requested stage connection id\n","                targetConnectionId = find_connection_id(devConnectionId = devConnectionId, targetStage=stage)\n","\n","                obj[key] = targetConnectionId\n","            else:\n","                update_pipeline_connections(value, stage)\n","    \n","    elif isinstance(obj, list):\n","        for item in obj:\n","            update_pipeline_connections(item, stage)\n","    \n","    # return pl_json"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5ffe41db-bbc3-42ff-b7e2-50d1e5624ff9"},{"cell_type":"code","source":["# function to parse the json of the pipeline and update the Warehouse and Lakehouse linked services\n","def update_linked_services(obj):\n","\n","    if isinstance(obj, dict):  # If the object is a dictionary\n","\n","        if \"linkedService\" in obj and isinstance(obj[\"linkedService\"], dict):\n","            properties = obj[\"linkedService\"].get(\"properties\", {})\n","            \n","            if properties.get(\"type\") == \"DataWarehouse\":\n","                \n","                type_properties = properties.get(\"typeProperties\", {})\n","\n","                # get the source values\n","                source_artifactId = type_properties.get(\"artifactId\", \"Not Found\")\n","                source_workspaceId = type_properties.get(\"workspaceId\", \"Not Found\")\n","                source_endpoint = type_properties.get(\"endpoint\", \"Not Found\")\n","\n","                # get the target values \n","                source_artifact_name = fabric.resolve_item_name(item_id=source_artifactId, workspace=vSourceWorkspaceId)\n","                target_artifact_id = fabric.resolve_item_id(item_name=source_artifact_name, type='Warehouse', workspace=vTargetWorkspaceId)\n","                artifact_url  = f\"v1/workspaces/{vTargetWorkspaceId}/warehouses/{target_artifact_id}\"\n","                response = client.get(artifact_url)\n","                target_endpoint = response.json()['properties']['connectionString']\n","                target_values = {\n","                    \"endpoint\": f\"{target_endpoint}\",\n","                    \"artifactId\": f\"{target_artifact_id}\",\n","                    \"workspaceId\": f\"{vTargetWorkspaceId}\"\n","                }\n","\n","                # update the properties using the target values\n","                type_properties[\"endpoint\"] = target_values[\"endpoint\"]\n","                type_properties[\"artifactId\"] = target_values[\"artifactId\"]\n","                type_properties[\"workspaceId\"] = target_values[\"workspaceId\"]\n","\n","            if properties.get(\"type\") == \"Lakehouse\":\n","                \n","                type_properties = properties.get(\"typeProperties\", {})\n","\n","                # get the source values\n","                source_artifactId = type_properties.get(\"artifactId\", \"Not Found\")\n","                source_workspaceId = type_properties.get(\"workspaceId\", \"Not Found\")\n","\n","\n","                # get the target values \n","                source_artifact_name = fabric.resolve_item_name(item_id = source_artifactId, workspace=vSourceWorkspaceId)\n","                target_artifact_id = fabric.resolve_item_id(item_name = source_artifact_name, type='Lakehouse', workspace=vTargetWorkspaceId)\n","                target_values = {\n","                    \"artifactId\": f\"{target_artifact_id}\",\n","                    \"workspaceId\": f\"{vTargetWorkspaceId}\"\n","                }\n","\n","                # update the properties using the target values\n","                type_properties[\"artifactId\"] = target_values[\"artifactId\"]\n","                type_properties[\"workspaceId\"] = target_values[\"workspaceId\"]\n","        \n","        # Recursively search all keys in the dictionary\n","        for key in obj:\n","            update_linked_services(obj[key])\n","    \n","    elif isinstance(obj, list):  # If the object is a list, iterate over elements\n","        for item in obj:\n","            update_linked_services(item)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"876f80b0-5c93-431e-9fb8-2293f7a682d2"},{"cell_type":"code","source":["# function to parse the json of the pipeline and update notebooks\n","def update_notebooks(obj):\n","    if isinstance(obj, dict):  # If the object is a dictionary\n","        if obj.get(\"type\") == \"TridentNotebook\":\n","            type_properties = obj.get(\"typeProperties\", {})\n","\n","            # get the source values\n","            source_notebook_id = type_properties.get(\"notebookId\", \"Not Found\")\n","            vSourceWorkspaceId = type_properties.get(\"workspaceId\", \"Not Found\")\n","\n","            # get the target values \n","            source_notebook_name = fabric.resolve_item_name(item_id=source_notebook_id, workspace=vSourceWorkspaceId)\n","            target_notebook_id = fabric.resolve_item_id(item_name=source_notebook_name, type='Notebook', workspace=vTargetWorkspaceId)\n","            target_values = {\n","                \"notebookId\": f\"{target_notebook_id}\",\n","                \"workspaceId\": f\"{vTargetWorkspaceId}\"\n","            }\n","\n","            # update the properties using the target values\n","            type_properties[\"notebookId\"] = target_values[\"notebookId\"]\n","            type_properties[\"workspaceId\"] = target_values[\"workspaceId\"]\n","\n","        # Recursively search all keys in the dictionary\n","        for key in obj:\n","            update_notebooks(obj[key])\n","\n","    elif isinstance(obj, list):  # If the object is a list, iterate over elements\n","        for item in obj:\n","            update_notebooks(item)\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"78be19ba-2fdf-4a02-85e9-46c5c0c9324b"},{"cell_type":"code","source":["# function to parse the json of the pipeline and update invoked fabric pipelines\n","def update_fabric_pipelines(obj):\n","    if isinstance(obj, dict):  # If the object is a dictionary\n","        if obj.get(\"type\") == \"InvokePipeline\":\n","            type_properties = obj.get(\"typeProperties\", {})\n","\n","            # get the source values\n","            operation_type = type_properties.get(\"operationType\", \"Not Found\")\n","\n","            if operation_type == \"InvokeFabricPipeline\":\n","                source_pipeline_id = type_properties.get(\"pipelineId\", \"Not Found\")\n","                vSourceWorkspaceId = type_properties.get(\"workspaceId\", \"Not Found\")\n","\n","                # get the target values \n","                source_pipeline_name = fabric.resolve_item_name(item_id=source_pipeline_id, workspace=vSourceWorkspaceId)\n","                target_pipeline_id = fabric.resolve_item_id(item_name=source_pipeline_name, type='DataPipeline', workspace=vTargetWorkspaceId)\n","                target_values = {\n","                    \"pipelineId\": f\"{target_pipeline_id}\",\n","                    \"workspaceId\": f\"{vTargetWorkspaceId}\"\n","                }\n","\n","                # update the properties using the target values\n","                type_properties[\"pipelineId\"] = target_values[\"pipelineId\"]\n","                type_properties[\"workspaceId\"] = target_values[\"workspaceId\"]\n","\n","        # Recursively search all keys in the dictionary\n","        for key in obj:\n","            update_fabric_pipelines(obj[key])\n","\n","    elif isinstance(obj, list):  # If the object is a list, iterate over elements\n","        for item in obj:\n","            update_fabric_pipelines(item)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9c7eb454-f25c-4317-9c7c-a7d0ebb8dab1"},{"cell_type":"code","source":["# function to parse the json of the pipeline and update semantic models\n","def update_semantic_models(obj):\n","    if isinstance(obj, dict):  # If the object is a dictionary\n","        if obj.get(\"type\") == \"PBISemanticModelRefresh\":\n","            type_properties = obj.get(\"typeProperties\", {})\n","\n","            # get the source values\n","            operation_type = type_properties.get(\"operationType\", \"Not Found\")\n","\n","            source_dataset_id = type_properties.get(\"datasetId\", \"Not Found\")\n","            vSourceWorkspaceId = type_properties.get(\"groupId\", \"Not Found\")\n","\n","            # get the target values \n","            source_dataset_name = fabric.resolve_item_name(item_id=source_dataset_id, workspace=vSourceWorkspaceId)\n","            target_dataset_id = fabric.resolve_item_id(item_name=source_dataset_name, type='SemanticModel', workspace=vTargetWorkspaceId)\n","            target_values = {\n","                \"datasetId\": f\"{target_dataset_id}\",\n","                \"groupId\": f\"{vTargetWorkspaceId}\"\n","            }\n","\n","            # update the properties using the target values\n","            type_properties[\"datasetId\"] = target_values[\"datasetId\"]\n","            type_properties[\"groupId\"] = target_values[\"groupId\"]\n","\n","        # Recursively search all keys in the dictionary\n","        for key in obj:\n","            update_semantic_models(obj[key])\n","\n","    elif isinstance(obj, list):  # If the object is a list, iterate over elements\n","        for item in obj:\n","            update_semantic_models(item)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e58bca5a-c9f9-4048-b111-bf652e497dcd"},{"cell_type":"code","source":["# function to update the data pipeline definition\n","def update_data_pipeline_definition(\n","    name: str, pipelineContent: dict, workspace: Optional[str] = None\n","):\n","    \"\"\"\n","    Updates an existing data pipeline with a new definition.\n","\n","    Parameters\n","    ----------\n","    name : str\n","        The name of the data pipeline.\n","    pipelineContent : dict\n","        The data pipeline content (not in Base64 format).\n","    workspace : str, default=None\n","        The name of the workspace.\n","        Defaults to None which resolves to the workspace of the attached lakehouse\n","        or if no lakehouse attached, resolves to the workspace of the notebook.\n","    \"\"\"\n","\n","    # resolve the workspace name and id\n","    (vWorkspace, vWorkspaceId) = resolve_workspace_name_and_id(workspace)\n","\n","    # get the pipeline payload\n","    vPipelinePayload = base64.b64encode(json.dumps(pipelineContent).encode('utf-8')).decode('utf-8')\n","    \n","    # resolve the pipeline id\n","    vPipelineId = fabric.resolve_item_id(item_name=name, type=\"DataPipeline\", workspace=vWorkspace)\n","\n","    # prepare the request body\n","    vRequestBody = {\n","        \"definition\": {\n","            \"parts\": [\n","                {\n","                    \"path\": \"pipeline-content.json\",\n","                    \"payload\": vPipelinePayload,\n","                    \"payloadType\": \"InlineBase64\"\n","                }\n","            ]\n","        }\n","    }\n","\n","    # response\n","    vResponse = client.post(\n","        f\"v1/workspaces/{vWorkspaceId}/items/{vPipelineId}/updateDefinition\",\n","        json=vRequestBody,\n","    )\n","\n","    lro(client, vResponse, return_status_code=True)\n","\n","    print(f\"{icons.green_dot} The '{name}' pipeline was updated within the '{vWorkspace}' workspace.\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"90bc6df9-4d7e-4145-9aca-6ec60ebf0a92"},{"cell_type":"markdown","source":["**Replacement of linked services, connections, notebooks, fabric pipelines, etc..**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b0b6f71a-0e44-4470-be6e-0bed5c50a0d3"},{"cell_type":"code","source":["\n","# get the list of data pipelines in the target workspace\n","df_pipeline = labs.list_data_pipelines(vTargetWorkspaceName)\n","\n","# iterate over the data pipelines\n","for index, row in df_pipeline.iterrows():\n","\n","    vPipelineName = row['Data Pipeline Name']\n","\n","    # retrieve the pipeline json definition\n","    vPipelineJson = json.loads(labs.get_data_pipeline_definition(vPipelineName, vSourceWorkspaceName))\n","    # print(json.dumps(vPipelineJson, indent=4))\n","\n","\n","    # update linked services\n","    try:\n","        update_linked_services(vPipelineJson.get(\"properties\", {}).get(\"activities\", []))\n","        \n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update linked services', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update linked services', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","            \n","\n","    # update connections\n","    try:\n","        update_pipeline_connections(vPipelineJson, pTargetStage)\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update connections', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update connections', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n","    # update notebooks\n","    try:\n","        update_notebooks(vPipelineJson)\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update notebooks', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update notebooks', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n","    # update fabric pipeline \n","    try:\n","        update_fabric_pipelines(vPipelineJson)\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update fabric pipeline', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update fabric pipeline', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n","    # update semantic models\n","    try:\n","        update_semantic_models(vPipelineJson)\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update semantic models', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update semantic models', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n","    # update pipeline definition\n","    try:\n","        update_data_pipeline_definition(name=vPipelineName,pipelineContent=vPipelineJson, workspace=vTargetWorkspaceName)\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update pipeline definition', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vPipelineName, 'update pipeline definition', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n","\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"773ca126-b15d-496d-b1b4-4796c665a7a1"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"451e555d-94ce-4881-8d35-56da120dee20"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging_cicd\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"10f47229-de50-4df4-869b-21d4c9b0f489"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}