{"cells":[{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50b46991-79b4-43ed-a3c1-45caa9767963"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:46.7763146Z","execution_start_time":"2025-02-23T14:03:37.4132795Z","parent_msg_id":"f6949997-599b-4d61-9300-2d50e981f38d","queued_time":"2025-02-23T14:03:37.2496055Z","livy_statement_state":"available","statement_ids":[82,83,84,85,86,87,88,89,90,91,92,93],"session_start_time":null,"normalized_state":"finished","statement_id":93},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 93, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Warning: In reference nb_helper run, the default lakehouse of the main notebook will be the effective default lakehouse of the session during reference run. Recommend using absolute path to read/write lakehouse in the referenced notebooks.\n"]}],"execution_count":47,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b7e81de0-32ed-4552-b145-399179df0802"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ae7a14a6-b3d7-4a75-9c26-df584197e083"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'SourceWorkspaceName','TargetWorkspaceName','Item', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:47.1741892Z","execution_start_time":"2025-02-23T14:03:46.9123716Z","parent_msg_id":"ff52f9bc-14c7-44f2-93d2-9baf9448e166","queued_time":"2025-02-23T14:03:36.9723489Z","livy_statement_state":"available","statement_ids":[94],"session_start_time":null,"normalized_state":"finished","statement_id":94},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 94, Finished, Available, Finished)"},"metadata":{}}],"execution_count":48,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1c029698-13c1-4981-9b99-239b79b7c42b"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e871a7f2-f46f-4d25-9f5e-4b482692db38"},{"cell_type":"markdown","source":["pSourceWorkspaceId = \"35f100c1-d910-482c-9763-aaf500918816\"\n","pTargetWorkspaceId = \"2fc80f23-1f8f-4c00-b2de-507863e8def4\"\n","pTargetStage = \"Stage3\"\n","pDebugMode = \"yes\"\n","pProjectName = \"fabric-cicd\"\n","pMappingConnections = '[{\"ConnectionStage0\":\"0c0702d4-9c1d-435a-a03e-9635e1fbded8\",\"ConnectionStage1\":\"0c0702d4-9c1d-435a-a03e-9635e1fbded8\",\"ConnectionStage2\":\"feb079dc-6fe7-4f0c-9537-33d7fa72fcb4\",\"ConnectionStage3\":\"feb079dc-6fe7-4f0c-9537-33d7fa72fcb4\"},{\"ConnectionStage0\":\"a24fefc1-e5f4-4606-a3e1-a337b7056627\",\"ConnectionStage1\":\"a24fefc1-e5f4-4606-a3e1-a337b7056627\",\"ConnectionStage2\":null,\"ConnectionStage3\":null},{\"ConnectionStage0\":\"0c0702d4-9c1d-435a-a03e-9635e1fbded8\",\"ConnectionStage1\":\"0c0702d4-9c1d-435a-a03e-9635e1fbded8\",\"ConnectionStage2\":null,\"ConnectionStage3\":null},{\"ConnectionStage0\":\"b8d19a81-9f45-4eed-aef0-314a28c1b16f\",\"ConnectionStage1\":\"b8d19a81-9f45-4eed-aef0-314a28c1b16f\",\"ConnectionStage2\":null,\"ConnectionStage3\":null},{\"ConnectionStage0\":\"2c52c32b-1d27-4de6-852c-9fd8be27cad1\",\"ConnectionStage1\":\"39e95e92-8338-4ad9-8a97-14b39388349b\",\"ConnectionStage2\":null,\"ConnectionStage3\":null},{\"ConnectionStage0\":\"Sql.Database(''rs-synapse-dev-ondemand.sql.azuresynapse.net'', ''misc'')\",\"ConnectionStage1\":\"Sql.Database(''rs-synapse-dev-ondemand.sql.azuresynapse.net'', ''misc'')\",\"ConnectionStage2\":\"Sql.Database(''rs-synapse-dev-ondemand.sql.azuresynapse.net'', ''misc_new'')\",\"ConnectionStage3\":\"Sql.Database(''rs-synapse-dev-ondemand.sql.azuresynapse.net'', ''misc_new'')\"}]'\n"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"8474d65d-0ead-43f6-93eb-418dc180d866"},{"cell_type":"markdown","source":["**Resolve source and target workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"01d01936-704f-4403-b7c9-7f75d3d6f8ad"},{"cell_type":"code","source":["vSourceWorkspaceName = fabric.resolve_workspace_name(pSourceWorkspaceId)\n","vTargetWorkspaceName = fabric.resolve_workspace_name(pTargetWorkspaceId)\n","vSourceWorkspaceId = pSourceWorkspaceId\n","vTargetWorkspaceId = pTargetWorkspaceId"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:47.9492584Z","execution_start_time":"2025-02-23T14:03:47.704142Z","parent_msg_id":"8d341e35-f272-40c2-8b08-0f71f26b0f90","queued_time":"2025-02-23T14:03:37.1789427Z","livy_statement_state":"available","statement_ids":[96],"session_start_time":null,"normalized_state":"finished","statement_id":96},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 96, Finished, Available, Finished)"},"metadata":{}}],"execution_count":50,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe985d3b-334d-44fa-af89-1df0aecefa5d"},{"cell_type":"markdown","source":["**List of semantic models in source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e5eb5ac2-276d-4a7c-9abd-e31e56e76532"},{"cell_type":"code","source":["df_source_semantic_models = fabric.list_datasets(workspace=vSourceWorkspaceName) "],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:48.84726Z","execution_start_time":"2025-02-23T14:03:48.5952192Z","parent_msg_id":"afa79b52-b75c-401c-9bfb-8e603a804f5d","queued_time":"2025-02-23T14:03:37.3474108Z","livy_statement_state":"available","statement_ids":[98],"session_start_time":null,"normalized_state":"finished","statement_id":98},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 98, Finished, Available, Finished)"},"metadata":{}}],"execution_count":52,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d2b105e2-4b0a-4791-bb75-0a42d846fc06"},{"cell_type":"markdown","source":["**Verify that there is a least one semantic model in the source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dea24dd2-f2d9-4464-9936-932afbf7a953"},{"cell_type":"code","source":["if df_source_semantic_models.empty:\n","    vMessage = f\"workspace <vSourceWorkspaceName> have 0 semantic model. post-update is not required.\"\n","\n","    # Display an exit message\n","    display(Markdown(\"### âœ… Notebook execution stopped successfully!\"))\n","\n","    # Exit without error\n","    mssparkutils.notebook.exit(vMessage)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:49.2619297Z","execution_start_time":"2025-02-23T14:03:49.0223076Z","parent_msg_id":"8068dba7-028b-4efe-9010-7e4151e5f5b2","queued_time":"2025-02-23T14:03:37.4649241Z","livy_statement_state":"available","statement_ids":[99],"session_start_time":null,"normalized_state":"finished","statement_id":99},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 99, Finished, Available, Finished)"},"metadata":{}}],"execution_count":53,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bd564dd7-00f5-4736-8026-f09a47900ede"},{"cell_type":"markdown","source":["**Get the connections mapping between Stages and list existing fabric connections**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"948cb91a-c635-4acd-9129-f2c8abc56142"},{"cell_type":"code","source":["# get the mapping of connections between stages\n","mapping_connections_json = json.loads(pMappingConnections)\n","df_mapping_connections = pd.DataFrame(mapping_connections_json)\n","\n","# get the list of existing connections in the tenant. the list will be used for lookups \n","df_existing_connections = labs.list_connections()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:50.2318672Z","execution_start_time":"2025-02-23T14:03:49.3877026Z","parent_msg_id":"382dff60-b8aa-4fbe-ad27-597797c2fff9","queued_time":"2025-02-23T14:03:37.5887578Z","livy_statement_state":"available","statement_ids":[100],"session_start_time":null,"normalized_state":"finished","statement_id":100},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 100, Finished, Available, Finished)"},"metadata":{}}],"execution_count":54,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"39c3ece4-ec94-4090-a77f-138adf4eaf9a"},{"cell_type":"markdown","source":["**Functions**\n","- validate_stage_connection_id\n","- find_connection_id\n","- update_partition_source_expression"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"11923075-3df4-48d2-9955-34b7cc3d2993"},{"cell_type":"code","source":["def validate_stage_connection_id(connectionId):\n","\n","    if connectionId in df_existing_connections['Connection Id'].values:\n","        vMessage = f\"connection id <{connectionId}> is valid>\"\n","        print(f\"{vMessage}\") \n","        vConnectionValidation = \"valid\"\n","    else:\n","        vMessage = f\"connection id <{connectionId}> is unvalid>\"\n","        print(f\"{vMessage}\") \n","        vConnectionValidation = \"unvalid\"\n","    return vConnectionValidation"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:50.6312457Z","execution_start_time":"2025-02-23T14:03:50.3567775Z","parent_msg_id":"4e030c86-1b48-4043-a670-810c90a82d2e","queued_time":"2025-02-23T14:03:37.6807067Z","livy_statement_state":"available","statement_ids":[101],"session_start_time":null,"normalized_state":"finished","statement_id":101},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 101, Finished, Available, Finished)"},"metadata":{}}],"execution_count":55,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e05e9dbd-8b38-4191-9406-532796c5a0d5"},{"cell_type":"code","source":["# function to find a connection id based on the target stage\n","# the csv provided with the mapping between stages is used, with the assomption of 4 stages maximun (dev, test, uat, prod)\n","def find_connection_id(devConnectionId, targetStage, validateConnection):\n","\n","    global df_mapping_connections\n","\n","    vMessage = f\"dev connection id is <{devConnectionId}>\"\n","    print(f\"{vMessage}\") \n","\n","    # filter the DataFrame based on a condition\n","    df_mapping_connections_filtered = df_mapping_connections[(df_mapping_connections['ConnectionStage1'] == devConnectionId)]\n","\n","    # extract the value of a target connection id\n","    # if the target connection cannot be found assign it the dev connection to avoid breaking the json definition of the pipeline\n","    if not df_mapping_connections_filtered.empty:\n","\n","        first_row = df_mapping_connections_filtered.iloc[0]  # Get the first matching row\n","\n","        if targetStage == \"Stage2\":\n","            targetConnectionId = first_row[\"ConnectionStage2\"]\n","        elif targetStage == \"Stage3\":\n","            targetConnectionId = first_row[\"ConnectionStage3\"]\n","        else:\n","            targetConnectionId = first_row[\"ConnectionStage4\"]\n","\n","        # if the stage column in the mapping has no value, assing NA\n","        targetConnectionId = \"NA\" if pd.isna(targetConnectionId) or targetConnectionId == \"\" else targetConnectionId\n","\n","\n","        if validateConnection == \"yes\":\n","\n","            # validate that the stage connection exists\n","            vConnectionValidation = validate_stage_connection_id(targetConnectionId)\n","\n","            # if the validation of the connection fails , keep the dev connection\n","            if vConnectionValidation == \"unvalid\":\n","                targetConnectionId = devConnectionId\n","\n","    else:\n","        \n","        vMessage = f\"no valid connection found in the mapping matching the condition, source connection will be kept.\"\n","        print(f\"{vMessage}\") \n","\n","        # assign the dev connection to the target connection\n","        targetConnectionId = devConnectionId\n","\n","\n","    # return the found values\n","    return targetConnectionId"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:51.0918565Z","execution_start_time":"2025-02-23T14:03:50.8249335Z","parent_msg_id":"628859f7-a06a-4703-8f67-2f45dfcb7af6","queued_time":"2025-02-23T14:03:37.8002077Z","livy_statement_state":"available","statement_ids":[102],"session_start_time":null,"normalized_state":"finished","statement_id":102},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 102, Finished, Available, Finished)"},"metadata":{}}],"execution_count":56,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1de4d5fa-459b-421b-92f9-566faa05aef4"},{"cell_type":"code","source":["# function for semantic models where the connection to the source system and database is an M code\n","def update_partition_source_expression(obj, targetStage):\n","    \n","    # iterate on tables \n","    for table in obj.get(\"model\", {}).get(\"tables\", []):\n","\n","        # iterate on partitions\n","        for partition in table.get(\"partitions\", []):\n","\n","            # extract the source and from the source the expression\n","            source = partition.get(\"source\", {})\n","            expression = source.get(\"expression\", [])\n","            \n","            # M expression are multi lines, iterate over lines and extract the pattern that matches \"Source = \"\n","            for i, line in enumerate(expression):\n","                \n","                # pattern\n","                vMatch = re.match(r'\\s*Source\\s*=\\s*(.*),', line)\n","                \n","                # if there is a match\n","                if vMatch:\n","\n","                    # set the indentation\n","                    vIndentation = \"    \"\n","\n","                    # get the connection\n","                    # Power BI has hundreds of connectors and each has specifics exprections\n","                    # extracting values based on each connector requires knowledge of the syntax\n","                    # for simplicity, extract the full expression\n","                    devConnectionId = vMatch.group(1).strip()\n","                    # print(devConnectionId)\n","\n","                    # get the mapping connection expression\n","                    targetConnectionId = find_connection_id(devConnectionId=devConnectionId, targetStage=targetStage, validateConnection = 'no')\n","                    print(f\"devConnectionId <{devConnectionId}>, targetConnectionId <{targetConnectionId}>\")\n","\n","                    # set the expression\n","                    expression[i] = f'{vIndentation}Source = {targetConnectionId},'\n","                    break  # stop the iteratin after the first match\n","    \n","    # return the \n","    return obj"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:03:51.5463324Z","execution_start_time":"2025-02-23T14:03:51.2167428Z","parent_msg_id":"b535c213-9a52-44d1-8592-7581e18dc78e","queued_time":"2025-02-23T14:03:37.9216279Z","livy_statement_state":"available","statement_ids":[103],"session_start_time":null,"normalized_state":"finished","statement_id":103},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 103, Finished, Available, Finished)"},"metadata":{}}],"execution_count":57,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f7aeb147-3b1b-4ccb-80d4-a54bfe4cca0e"},{"cell_type":"markdown","source":["**Update direct lake model lakehouse connection**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"31638131-4528-4305-a2fd-cd29955d6b58"},{"cell_type":"code","source":["# get the list of semantic models in the workspace\n","df_target_semantic_models = fabric.list_datasets(workspace=vTargetWorkspaceName)\n","\n","# iterate over each dataset in the dataframe\n","for index, row in df_target_semantic_models.iterrows():\n","\n","    # get the semantic model name\n","    vSemanticModelName = row['Dataset Name']\n","\n","\n","    # update the connection of semantic models \n","    try:\n","\n","        # Check if the dataset is not the default semantic model\n","        if not labs.is_default_semantic_model(vSemanticModelName, vTargetWorkspaceId):\n","            \n","            print(f'updating semantic model <{vSemanticModelName}> connection in workspace <{vTargetWorkspaceName}>.')\n","\n","            # check if the semantic model has a direct lake lakehouse\n","            try:\n","                vDatasetDirectLakehouse=labs.directlake.get_direct_lake_lakehouse(\n","                    dataset=vSemanticModelName, \n","                    workspace= vTargetWorkspaceName,\n","                )\n","                vValidationDirectLake = \"valid\"\n","            \n","            except Exception as e:\n","                if \"SQL Endpoint not found\" in str(e):\n","                    vValidationDirectLake = \"unvalid\"\n","                    \n","\n","            # if the semantic lake has a direct lake lakehouse, update the connection and refresh it\n","            if vValidationDirectLake == \"valid\":\n","\n","                print(f'semantic model <{vSemanticModelName}> has a direct lake connection. using model.bim instead')\n","                \n","                # update the connection\n","                labs.directlake.update_direct_lake_model_connection(\n","                    dataset=vSemanticModelName, \n","                    workspace= vTargetWorkspaceName,\n","                    source=labs.directlake.get_direct_lake_source(vSemanticModelName, workspace=vTargetWorkspaceName)[1], \n","                    source_type=labs.directlake.get_direct_lake_source(vSemanticModelName, workspace=vTargetWorkspaceName)[0], \n","                    source_workspace=vTargetWorkspaceName\n","                )\n","                \n","                # refresh the semantic mode (metadata only)\n","                labs.refresh_semantic_model(dataset=vSemanticModelName, workspace=vTargetWorkspaceName)\n","\n","            else:\n","                print(f'semantic model <{vSemanticModelName}> has no direct lake connection. using the json structure instead')\n","\n","                # get the current definition as in the source workspace\n","                semantic_model_json = labs.get_semantic_model_bim(dataset=vSemanticModelName, workspace=vSourceWorkspaceName)\n","\n","                # print(json.dumps(semantic_model_json, indent=4))\n","\n","                # replace M expressions using the connection mapping\n","                semantic_model_json_new = update_partition_source_expression(semantic_model_json, pTargetStage)\n","\n","                \n","                # update the semantic model from the new json structure\n","                labs.update_semantic_model_from_bim(\n","                    dataset=vSemanticModelName, \n","                    bim_file=semantic_model_json_new, \n","                    workspace=vTargetWorkspaceName\n","                )\n","    \n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vSemanticModelName, 'update semantic model connection', datetime.now(), None, vMessage, ''] \n","    \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vSemanticModelName, 'update semantic model connection', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"session_id":"220708dc-1330-4d2e-a379-bb193810ff23","spark_pool":null,"state":"finished","execution_finish_time":"2025-02-23T14:05:51.602706Z","execution_start_time":"2025-02-23T14:05:43.1046401Z","parent_msg_id":"6334b920-0736-4863-8162-ed3813d9ee5a","queued_time":"2025-02-23T14:05:42.7362932Z","livy_statement_state":"available","statement_ids":[105],"session_start_time":null,"normalized_state":"finished","statement_id":105},"text/plain":"StatementMeta(, 220708dc-1330-4d2e-a379-bb193810ff23, 105, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["updating semantic model <AdventureWorks> connection in workspace <ci-cd-prod-05>.\nsemantic model <AdventureWorks> has no direct lake connection. using the json structure instead\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 51), match='    Source = Sql.Database(\".\", \"AdventureWorksDW\">\ndev connection id is <Sql.Database(\".\", \"AdventureWorksDW\")>\nSql.Database(\".\", \"AdventureWorksDW\") Sql.Database(\"localhost\", \"AdventureWorksDW\")\nNone\n<re.Match object; span=(0, 82), match='    Source = Sql.Database(\"rs-synapse-dev-ondeman>\ndev connection id is <Sql.Database(\"rs-synapse-dev-ondemand.sql.azuresynapse.net\", \"misc\")>\nSql.Database(\"rs-synapse-dev-ondemand.sql.azuresynapse.net\", \"misc\") Sql.Database(\"rs-synapse-dev-ondemand.sql.azuresynapse.net\", \"misc_new\")\nNone\n<re.Match object; span=(0, 189), match='    Source = Csv.Document(File.Contents(\"C:\\\\User>\ndev connection id is <Csv.Document(File.Contents(\"C:\\Users\\rsayegh\\OneDrive - Microsoft\\Azure_Training\\PowerBI\\datasets\\test_csv_source.csv\"),[Delimiter=\",\", Columns=2, QuoteStyle=QuoteStyle.None])>\nno valid connection found in the mapping matching the condition, source connection will be kept.\nCsv.Document(File.Contents(\"C:\\Users\\rsayegh\\OneDrive - Microsoft\\Azure_Training\\PowerBI\\datasets\\test_csv_source.csv\"),[Delimiter=\",\", Columns=2, QuoteStyle=QuoteStyle.None]) Csv.Document(File.Contents(\"C:\\Users\\rsayegh\\OneDrive - Microsoft\\Azure_Training\\PowerBI\\datasets\\test_csv_source.csv\"),[Delimiter=\",\", Columns=2, QuoteStyle=QuoteStyle.None])\nðŸŸ¢ The 'AdventureWorks' semantic model has been updated within the 'ci-cd-prod-05' workspace.\n"]}],"execution_count":59,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":false}},"id":"6d2502de-366c-4dee-bbbd-e477f3b7bf06"},{"cell_type":"markdown","source":["**Rebind reports**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d82934ea-5272-43be-9087-8f976eac8b99"},{"cell_type":"code","source":["# get the list of reports\n","df_target_reports = fabric.list_reports(workspace=vTargetWorkspaceName)\n","\n","# iterate over the reports\n","for index, row in df_target_reports.iterrows():\n","\n","    # get the semantic model name\n","    vReportName = row['Name']\n","    vSemanticModelId = row['Dataset Id']\n","    vSemanticModelName = fabric.resolve_item_name(item_id=vSemanticModelId, workspace=vTargetWorkspaceName)\n","\n","    # update report connection\n","    try:\n","        print(f'rebinding report <{vReportName}> to semantic model <{vSemanticModelName}> in workspace <{vTargetWorkspaceName}>.')\n","\n","        labs.report.report_rebind(\n","            report=vReportName,\n","            dataset=vSemanticModelName, \n","            report_workspace=vTargetWorkspaceName, \n","            dataset_workspace=vTargetWorkspaceName\n","        )\n","\n","\n","        vMessage = f\"succeeded\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vReportName, 'update report connection', datetime.now(), None, vMessage, ''] \n","    except Exception as e:\n","        vMessage = f\"failed\"\n","        dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, vSourceWorkspaceName, vTargetWorkspaceName, vReportName, 'update report connection', datetime.now(), None, vMessage, str(e) ] \n","        if pDebugMode == \"yes\":\n","            print(str(e))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"7c850199-4380-4369-8dba-a6631136d052"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"82f8bea3-217b-490a-8fb3-8cd3eacd6fba"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"SourceWorkspaceName\" : \"string\",\n","            \"TargetWorkspaceName\" : \"string\",\n","            \"Item\":\"string\",\n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging_cicd\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9d6d6709-0ac8-482a-a8ef-be2ddcb4ce2a"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"widgets":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"d1073252-d492-4168-85ee-9dc395278b29","known_lakehouses":[{"id":"d1073252-d492-4168-85ee-9dc395278b29"}],"default_lakehouse_name":"cicdlakehouse","default_lakehouse_workspace_id":"4d4452c6-7faf-46b4-81fc-21f5bcc6bd42"},"environment":{}}},"nbformat":4,"nbformat_minor":5}