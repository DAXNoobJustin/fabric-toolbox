{"cells":[{"cell_type":"markdown","source":["**Libraries**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2ee3b3a-f369-4f98-8e88-a04e999870cf"},{"cell_type":"code","source":["import pandas as pd\n","from datetime import datetime, timedelta\n","import sempy.fabric as fabric"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"151a5189-710d-4f0a-ad72-5ad9c422c817"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4788f8ff-57e3-44dc-82d2-225b1f99ba38"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"121fc594-15d2-47d2-b4d0-f9609f50791b"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f8421d0a-f588-4f23-bd13-4696e6f2fcc5"},{"cell_type":"markdown","source":["pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pTargetStage = \"Stage1\"\n","pDebugMode = \"yes\"\n","pTimeoutPerCellInSeconds = \"600\"\n","pTimeoutInSeconds = \"900\"\n","pProjectName = \"fabric-cicd\"\n","pFeatureBranch = \"NA\"\n","pMappingConnections = ''"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"22da0c79-bfea-43ea-9d57-583771c4f22f"},{"cell_type":"markdown","source":["**Check if the source workspace passed from DevOps equals the feature branch name**\n","- This is a specific handling when a PR is done from the feature branch"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"331987f9-7061-4338-8e4a-934f0974fb5a"},{"cell_type":"code","source":["if pSourceWorkspaceId == pFeatureBranch:\n","    pSourceWorkspaceId = fabric.resolve_workspace_id(workspace=pFeatureBranch)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"209a146b-fa1c-41bc-b05a-e258608ae3b9"},{"cell_type":"markdown","source":["**Define the DAG**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6a8b6198-1624-4008-9f6d-a4133b4650ab"},{"cell_type":"code","source":["dagList = []\n","\n","# add to the DAG list nb_cicd_post_update_data_pipelines\n","dagList.append({\n","            \"name\": \"nb_cicd_post_update_data_pipelines\",\n","            \"path\": \"nb_cicd_post_update_data_pipelines\",\n","            \"timeoutPerCellInSeconds\": int(pTimeoutPerCellInSeconds),\n","            \"args\": {\n","                \"useRootDefaultLakehouse\": True,\n","                \"pSourceWorkspaceId\":pSourceWorkspaceId,\n","                \"pTargetWorkspaceId\":pTargetWorkspaceId,\n","                \"pTargetStage\":pTargetStage,\n","                \"pDebugMode\":pDebugMode,\n","                \"pProjectName\":pProjectName,\n","                \"pMappingConnections\": pMappingConnections\n","                }\n","        })\n","\n","# add to the DAG list nb_cicd_post_update_notebooks\n","dagList.append({\n","            \"name\": \"nb_cicd_post_update_notebooks\",\n","            \"path\": \"nb_cicd_post_update_notebooks\",\n","            \"timeoutPerCellInSeconds\": int(pTimeoutPerCellInSeconds),\n","            \"args\": {\n","                \"useRootDefaultLakehouse\": True,\n","                \"pSourceWorkspaceId\":pSourceWorkspaceId,\n","                \"pTargetWorkspaceId\":pTargetWorkspaceId,\n","                \"pDebugMode\":pDebugMode\n","                }\n","        })\n","\n","# add to the DAG list nb_cicd_post_update_semantic_models\n","dagList.append({\n","            \"name\": \"nb_cicd_post_update_semantic_models\",\n","            \"path\": \"nb_cicd_post_update_semantic_models\",\n","            \"timeoutPerCellInSeconds\": int(pTimeoutPerCellInSeconds),\n","            \"args\": {\n","                \"useRootDefaultLakehouse\": True,\n","                \"pSourceWorkspaceId\":pSourceWorkspaceId,\n","                \"pTargetWorkspaceId\":pTargetWorkspaceId,\n","                \"pTargetStage\":pTargetStage,\n","                \"pDebugMode\":pDebugMode,\n","                \"pProjectName\":pProjectName,\n","                \"pMappingConnections\": pMappingConnections\n","                }\n","        })\n","\n","DAG = { \"activities\": dagList,\"concurrency\": 1, \"timeoutInSeconds\": int(pTimeoutInSeconds) }\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cb40613c-1d2b-4edf-8d1e-de25de315fec"},{"cell_type":"markdown","source":["**Run multiple**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3345dfe0-84c7-4c33-9742-b726e28ef82e"},{"cell_type":"code","source":["try:\n","    mssparkutils.notebook.runMultiple(DAG, {\"displayDAGViaGraphviz\": True})\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'running the DAG', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'running the DAG', datetime.now(), None, vMessage, str(e)]\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"21fdc174-a457-4bf3-9b38-5fb7c8d8a68d"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"89352377-24c6-4f57-9ba1-18532e7587ba"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2daf311c-d307-4ddd-83eb-0d7853214cc8"},{"cell_type":"markdown","source":["**Exit notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4443d25b-5e32-4a66-baf0-4e6da76595ef"},{"cell_type":"code","source":["mssparkutils.notebook.exit(f\"Notebook <{vLogNotebookName}> run successfully. Check logging table in CI/CD lakehouse for more details.\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e97c7158-a8fa-4913-84a0-024246cf9275"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"widgets":{},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}