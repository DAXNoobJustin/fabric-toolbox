{"cells":[{"cell_type":"markdown","source":["**This notebook will do the following:**\n","- Create the required folders in the File section\n","- Bind the cicd notebooks to the cicd lakehouse"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fe0183e9-77ea-4029-9444-ba65d61bba9a"},{"cell_type":"markdown","source":["**Manual step**\n","- Manually create a schema enabled cicd lakehouse \n","- Remove the existing attached lakehouse and attach the newly created cicd lakehouse to this notebook"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ed6598db-a7be-4e69-8171-869553c8c998"},{"cell_type":"markdown","source":["**Variables**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"748fefea-2fd2-4c57-b901-56a0e8ae831e"},{"cell_type":"code","source":["vLakehouseName = \"cicdlakehouse\" # the name of your lakehouse\n","vCicdFolderName = \"cicd\"\n","vConnectionFolderName = \"connections\" # folder to host the extraction of exiting connections \n","vOnelakeAccessFolderName = \"onelake_access\"\n","vProjectName = \"fabric-cicd\" # replace by your ci\n","vDebugMode = \"yes\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"55e061f2-bcf0-4ddd-bc21-fc3a88672028"},{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"099e227e-1d3e-4e53-929f-0a8b4b2e6dbb"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a02c4686-cc0f-411b-a084-8cec727db996"},{"cell_type":"markdown","source":["**Token and base url**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d1351226-3083-4b4d-950d-0926db725ae6"},{"cell_type":"code","source":["vApiVersion = \"v1\"\n","vScope = \"https://analysis.windows.net/powerbi/api\"\n","vAccessToken  = mssparkutils.credentials.getToken(vScope)\n","vBaseUrl = f\"https://api.fabric.microsoft.com/{vApiVersion}/\"\n","vHeaders = {'Authorization': f'Bearer {vAccessToken}'}"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6cc2f2f2-698f-4ba7-aadb-d954225d57e4"},{"cell_type":"markdown","source":["**Resolve current workspace name and id**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eb7e1857-8aed-4dff-bd8d-4c2f8d558f87"},{"cell_type":"code","source":["vWorkspaceName, vWorkspaceId = fabric.resolve_workspace_name_and_id()\n","print(vWorkspaceName, vWorkspaceId)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f11a7548-6085-4e0b-ba7d-51aa10a836a3"},{"cell_type":"markdown","source":["**Create a schema called staging**"],"metadata":{"nteract":{"transient":{"deleting":false}}},"id":"646661c7-8b91-466f-9187-b12b3b229100"},{"cell_type":"code","source":["%%sql\n","CREATE SCHEMA staging"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"6580b0c9-a7ce-48e2-b61c-4206ee3bb403"},{"cell_type":"markdown","source":["**Create the folders in the lakehouse**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3bc41d14-d70d-4c51-9fe9-ece7730a90d6"},{"cell_type":"code","source":["# this folder can be used to host a file containing the list of existing connections in a fabric tenant\n","# the list can help creating the mapping_connections.json secure file required for running post deployment steps in the yaml pipeline\n","# it is not specific to a particular project\n","notebookutils.fs.mkdirs(f\"Files/{vCicdFolderName}/{vConnectionFolderName}\")  \n","\n","# this folder can be use to host the onelake access files generated by nb_extract_lakehouse_access\n","# these files can help the creation of dedicated roles in target lakehouses \n","# the folder should be specific to a project\n","notebookutils.fs.mkdirs(f\"Files/{vCicdFolderName}/{vOnelakeAccessFolderName}/{vProjectName}\") "],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3fc003cd-763a-4950-9b7e-ecf7098b9450"},{"cell_type":"markdown","source":["**Manual step**\n","- For all projects --> extract the list of connections in the tenant\n","- Per project --> run nb_extract_lakehouse_access to extract the onelake roles definition for a specific project"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"08220090-7bfb-4900-ae28-ca955e489868"},{"cell_type":"markdown","source":["**Define the default lakehouse in the cicd notebooks:**\n","- nb_cicd_post_deployment\n","- nb_cicd_post_update_data_pipelines\n","- nb_cicd_post_update_notebooks\n","- nb_cicd_post_update_semantic_models\n","- nb_cicd_pre_deployment\n","- nb_cicd_pre_update_lakehouses\n","- nb_cicd_pre_update_warehouses"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"77b859b4-422b-4c02-b46a-e9d1991409bb"},{"cell_type":"code","source":["vLakehouseId = fabric.resolve_item_id(vLakehouseName)\n","\n","# get the list of data pipelines in the target workspace\n","vNotebookList = [\n","    'nb_cicd_post_deployment',\n","    'nb_cicd_post_update_data_pipelines',\n","    'nb_cicd_post_update_notebooks',\n","    'nb_cicd_post_update_semantic_models',\n","    'nb_cicd_pre_deployment',\n","    'nb_cicd_pre_update_lakehouses',\n","    'nb_cicd_pre_update_warehouses'\n","]\n","\n","df_notebooks = notebookutils.notebook.list(workspaceId=vWorkspaceId)\n","for notebook in df_notebooks:\n","    \n","    # get the notebook id and display name\n","    vNotebookId = notebook.id\n","    vNotebookName = notebook.displayName\n","\n","    if vNotebookName in vNotebookList:\n","\n","        # get the current notebook definition\n","        vNotebookDefinition = notebookutils.notebook.getDefinition(name=vNotebookName, workspaceId=vWorkspaceId) \n","        vNotebookJson = json.loads(vNotebookDefinition)\n","\n","        # update lakehouse dependencies\n","        try:\n","\n","            # check and remove any attached lakehouses\n","            if 'dependencies' in vNotebookJson['metadata'] \\\n","                and 'lakehouse' in vNotebookJson['metadata']['dependencies'] \\\n","                and vNotebookJson['metadata'][\"dependencies\"][\"lakehouse\"] is not None:\n","\n","                vCurrentLakehouse = vNotebookJson['metadata']['dependencies']['lakehouse']\n","                # print(vCurrentLakehouse)\n","\n","                if 'default_lakehouse_name' in vCurrentLakehouse:\n","\n","                    vNotebookJson['metadata']['dependencies']['lakehouse'] = {}\n","                    print(f\"attempting to update notebook <{vNotebookName}> with new default lakehouse: {vCurrentLakehouse['default_lakehouse_name']} in workspace <{vWorkspaceName}>.\")\n","\n","                    # update new notebook definition after removing existing lakehouses and with new default lakehouseId\n","                    notebookutils.notebook.updateDefinition(\n","                        name = vNotebookName,\n","                        content  = json.dumps(vNotebookJson),  \n","                        defaultLakehouse = vLakehouseName, #vCurrentLakehouse['default_lakehouse_name'],\n","                        defaultLakehouseWorkspace = vWorkspaceId,\n","                        workspaceId = vWorkspaceId\n","                    )\n","\n","                    print(f\"updated notebook <{vNotebookName}> in workspace <{vWorkspaceName}>.\")\n","\n","                else:\n","                    print(f'no default lakehouse set for notebook <{vNotebookName}>, ignoring.')\n","\n","            vMessage = f\"succeeded\"\n","        except Exception as e:\n","            vMessage = f\"failed\"\n","            if vDebugMode == \"yes\":\n","                print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1a9f2c16-2d5c-4e02-b600-b7290e71b5fb"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{}}},"nbformat":4,"nbformat_minor":5}