{"cells":[{"cell_type":"markdown","source":["**Helper notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"da95792c-4c46-4183-be8b-5b0e6b0f9ef9"},{"cell_type":"code","source":["%run nb_helper"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b563414c-45f3-438c-b444-586d71b386d0"},{"cell_type":"markdown","source":["**Define a logging dataframe**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c2c41a93-d0ff-4763-bf57-7269190714e0"},{"cell_type":"code","source":["dfLogging = pd.DataFrame(columns = ['LoadId','NotebookId', 'NotebookName', 'WorkspaceId', 'CellId', 'Timestamp', 'ElapsedTime', 'Message', 'ErrorMessage'])\n","vContext = mssparkutils.runtime.context\n","vNotebookId = vContext[\"currentNotebookId\"]\n","vLogNotebookName = vContext[\"currentNotebookName\"]\n","vWorkspaceId = vContext[\"currentWorkspaceId\"] # where the notebook is running, to not confuse with source and target workspaces"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9ac8aeb9-3878-479a-ae29-640161732bc1"},{"cell_type":"markdown","source":["**Parameters --> convert to code for debugging the notebook. otherwise, keep commented as parameters are passed from DevOps pipelines**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"de3479eb-28dd-4ad6-b1a2-35466e20a214"},{"cell_type":"code","source":["\n","pSqlToken = \"\"\n","pSourceWorkspaceId = \"\"\n","pTargetWorkspaceId = \"\"\n","pDebugMode = \"yes\""],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"50083b29-bc68-42a8-b289-66e4867320c5"},{"cell_type":"markdown","source":["**Resolve source and target workspace ids**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0bb45b09-4d2d-47cc-b665-31a4b9575918"},{"cell_type":"code","source":["vSourceWorkspaceName = fabric.resolve_workspace_name(pSourceWorkspaceId)\n","vTargetWorkspaceName = fabric.resolve_workspace_name(pTargetWorkspaceId)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5d6326ab-345b-4d5b-871b-a61d08a522c4"},{"cell_type":"markdown","source":["**List source and target warehouses**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"3cacbc51-47fe-4a0b-a0cb-5df287ceb2f5"},{"cell_type":"code","source":["df_source_warehouses = labs.list_warehouses(workspace=vSourceWorkspaceName)\n","df_target_warehouses = labs.list_warehouses(workspace=vTargetWorkspaceName)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"eef84dce-fbc2-4566-b4a8-c1e98f4e0514"},{"cell_type":"markdown","source":["**Verify that there is a least one warehouse in the source or the target workspace --> if there are no warehouses, exit the notebook**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"43ee39c4-d1de-4fd6-ab5b-08a4e87bbd4b"},{"cell_type":"code","source":["if df_target_warehouses.empty or df_source_warehouses.empty:\n","    vMessage = f\"workspace <vSourceWorkspaceName> or workspace <vTargetWorkspaceName> have 0 warehouse. pre-update is not required\"\n","    print(vMessage)\n","\n","\n","    # Display an exit message\n","    display(Markdown(\"### âœ… Notebook execution stopped successfully!\"))\n","\n","    # Exit without error\n","    # sys.exit(0)\n","    # InteractiveShell.instance().ask_exit()\n","    mssparkutils.notebook.exit(vMessage)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b25ea850-66aa-422d-873f-efb4a37cc3f3"},{"cell_type":"markdown","source":["**Source and target sql analytics endpoints**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"867d321e-85e1-4a30-b21c-09da7ae0206e"},{"cell_type":"code","source":["vSourceSqlEndpoint = df_source_warehouses.loc[0, 'Connection Info']\n","vTargetSqlEndpoint = df_target_warehouses.loc[0, 'Connection Info']"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"4d85c781-45aa-414e-a1ad-51e16095064c"},{"cell_type":"markdown","source":["**Access Token**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"9aac2c16-29ff-4797-893f-7516002e5dc5"},{"cell_type":"code","source":["vScope = \"https://analysis.windows.net/powerbi/api\"\n","\n","# get the access token \n","if pDebugMode == \"yes\":\n","    # in debug mode, use the token of the current user\n","    vSqlAccessToken  = mssparkutils.credentials.getToken(vScope)\n","else:\n","    # when the code is run from DevOps, the token passed as a parameter\n","    vSqlAccessToken = pSqlToken"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e59e65a8-4156-4992-b7b9-2f682439d3f5"},{"cell_type":"markdown","source":["**Sql statement to get the tables and their columns**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"be2f4c31-195a-46e3-863b-ac1b508289d9"},{"cell_type":"code","source":["vSqlStatement = \"\"\"\n","SELECT \n","\t DB_NAME() as [DATABASE_NAME]\n","\t,c.TABLE_SCHEMA\n","\t,c.TABLE_NAME\n","\t,c.ORDINAL_POSITION\n","\t,c.COLUMN_NAME\n","\t, \n","\t'[' + DATA_TYPE + ']'\n","\t+ \n","\tCASE  \n","\t\tWHEN DATA_TYPE IN ('tinyint', 'smallint', 'int', 'bigint','xml', 'smalldatetime', 'datetime', 'datetime2', 'bit', 'date', 'money', 'float', 'real') THEN ''\n","\t\tWHEN DATA_TYPE IN ('varchar', 'nvarchar', 'nchar', 'varbinary', 'char') \n","\t\tTHEN \n","\t\t\t'(' \n","\t\t\t+ \n","\t\t\tCASE CHARACTER_MAXIMUM_LENGTH \n","\t\t\t\tWHEN -1 THEN 'max'\n","\t\t\t\tELSE CAST(CHARACTER_MAXIMUM_LENGTH AS VARCHAR(10))\n","\t\t\tEND \n","\t\t\t+ ')'\n","\t\tWHEN DATA_TYPE IN ('numeric', 'decimal') THEN '(' + CAST(NUMERIC_PRECISION AS VARCHAR(10)) + ',' + CAST(NUMERIC_SCALE AS VARCHAR(10)) + ')'\n","\tEND \n","\tAS COLUMN_DEFINITION\n","FROM \n","\tINFORMATION_SCHEMA.COLUMNS c\n","\tINNER JOIN INFORMATION_SCHEMA.TABLES t \n","\t\tON c.TABLE_NAME = t.TABLE_NAME AND t.TABLE_TYPE = 'BASE TABLE'\n","WHERE\t\n","\tc.TABLE_SCHEMA NOT IN ('INFORMATION_SCHEMA','queryinsights','sys')\n","ORDER BY \n","\tc.TABLE_SCHEMA\n","\t,c.TABLE_NAME\n","\t,c.ORDINAL_POSITION\n","\"\"\""],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"550105fc-9579-4c6a-a048-a8d5a4d5ec32"},{"cell_type":"markdown","source":["**Functions**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d5e4909-aa21-4732-8645-38ffb6019ffe"},{"cell_type":"code","source":["# compare source and target dataframes, identify new and modified columns\n","def compare_dataframes(source_dataframe, target_dataframe, key_columns):\n","\n","    # Ensure both DataFrames have the same columns\n","    assert list(target_dataframe.columns) == list(source_dataframe.columns), \"DataFrames must have the same columns\"\n","\n","    source_dataframe_indexed = source_dataframe.set_index(key_columns)\n","    target_dataframe_indexed = target_dataframe.set_index(key_columns)\n","\n","    # columns in source but not in target --> added to source\n","    df_columns_only_in_source = source_dataframe_indexed.loc[~source_dataframe_indexed.index.isin(target_dataframe_indexed.index)].reset_index()\\\n","\n","    # # rows in target but not in source --> deleted from source\n","    # columns_only_in_target = target_dataframe_indexed.loc[~target_dataframe_indexed.index.isin(source_dataframe_indexed.index)].reset_index()\n","\n","    # columns in common but with a data type change\n","    df_common_rows = target_dataframe_indexed.index.intersection(source_dataframe_indexed.index)\n","    columns_with_type_change_list = []\n","    for index in df_common_rows:\n","            if not target_dataframe_indexed.loc[index].equals(source_dataframe_indexed.loc[index]):  # Compare row values\n","                modified_row = source_dataframe_indexed.loc[[index]].reset_index()  # Fetch modified row from B\n","                # modified_row[\"Change_Type\"] = \"modified\"\n","                columns_with_type_change_list.append(modified_row)\n","\n","    df_columns_with_type_change = pd.concat(columns_with_type_change_list, ignore_index=True) if columns_with_type_change_list else pd.DataFrame()\n","\n","    return df_columns_only_in_source, df_columns_with_type_change"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b8259469-c19e-45ee-8a9d-1e32c9523ac4"},{"cell_type":"code","source":["# create the alchemy engine\n","def create_sqlalchemy_engine(connection_string : str):\n","    token = pSqlToken\n","    SQL_COPT_SS_ACCESS_TOKEN = 1256\n","\n","    # the following code is required to structure the token for pyodbc.connect\n","    exptoken = b'';\n","    for i in bytes(token, \"UTF-8\"):\n","        exptoken += bytes({i});\n","        exptoken += bytes(1);\n","    tokenstruct = struct.pack(\"=i\", len(exptoken)) + exptoken;\n","\n","    return sqlalchemy.create_engine(\"mssql+pyodbc://\", creator=lambda: pyodbc.connect(connection_string, attrs_before = { SQL_COPT_SS_ACCESS_TOKEN:bytearray(tokenstruct) }))"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"317fa76a-954f-44f4-a778-f5714a068056"},{"cell_type":"markdown","source":["**Get the definition of warehouse(s) tables in the source workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c55d7dd5-36b5-4114-81c8-15088f7593b3"},{"cell_type":"code","source":["df_source_warehouses_columns = pd.DataFrame()\n","try:\n","\n","    for index, row in df_source_warehouses.iterrows():\n","\n","        # get the current warehouse\n","        vWarehouseName = row['Warehouse Name']\n","\n","        # define the connection string for the alchemy engine\n","        vConnectionString = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={vSourceSqlEndpoint};Database={vWarehouseName};\"\n","        # print(vConnectionString)\n","\n","        # create the sql engine\n","        sql_engine = create_sqlalchemy_engine(vConnectionString)\n","\n","        # connect to the engine\n","        with sql_engine.connect() as sql_connection:\n","\n","            # get the definition of the tables\n","            df_source_warehouses_columns_temp = pd.read_sql(vSqlStatement, sql_connection)\n","\n","            # append the rows to the dataframe\n","            df_source_warehouses_columns = pd.concat([df_source_warehouses_columns, df_source_warehouses_columns_temp], ignore_index=True)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source warehouses tables definition', datetime.now(), None, vMessage, ''] \n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify source warehouses tables definition', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d668d0da-a791-4980-8bcc-c9ed2e04d341"},{"cell_type":"markdown","source":["**Get the definition of warehouse(s) tables in the target workspace**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"a8b319cc-51b7-4b60-9c3b-1c8b537725c3"},{"cell_type":"code","source":["df_target_warehouses_columns = pd.DataFrame()\n","try:\n","\n","    for index, row in df_target_warehouses.iterrows():\n","\n","        # get the current warehouse\n","        vWarehouseName = row['Warehouse Name']\n","\n","        # define the connection string for the alchemy engine\n","        vConnectionString = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={vTargetSqlEndpoint};Database={vWarehouseName}\"\n","\n","        # create the sql engine\n","        sql_engine = create_sqlalchemy_engine(vConnectionString)\n","\n","        # connect to the engine\n","        with sql_engine.connect() as sql_connection:\n","\n","            # get the definition of the tables\n","            df_target_warehouses_columns_temp = pd.read_sql(vSqlStatement, sql_connection)\n","\n","            # append the rows to the dataframe\n","            df_target_warehouses_columns = pd.concat([df_target_warehouses_columns, df_target_warehouses_columns_temp], ignore_index=True)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify target warehouses tables definition', datetime.now(), None, vMessage, ''] \n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'identify target warehouses tables definition', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fc5c9a5e-7e97-43c3-9fa2-1c5386cb00a4"},{"cell_type":"markdown","source":["**Build the logic for the sql statements**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"05658626-30ac-4f21-8b0c-e1d90d797c77"},{"cell_type":"code","source":["# key columns for the merge and comparison\n","key_columns = ['DATABASE_NAME', 'TABLE_SCHEMA', 'TABLE_NAME', 'COLUMN_NAME']\n","\n","# source and target comparison\n","df_columns_only_in_source, df_columns_with_type_changed = compare_dataframes(df_source_warehouses_columns, df_target_warehouses_columns, key_columns)\n","df_compare_dataframes_unioned = pd.concat([df_columns_only_in_source, df_columns_with_type_changed]).drop_duplicates()\n","\n","if not df_compare_dataframes_unioned.empty:\n","    df_columns_in_common_not_changed = df_source_warehouses_columns.merge(df_compare_dataframes_unioned, on=list(df_source_warehouses_columns.columns), how='left', indicator=True).query('_merge == \"left_only\"').drop('_merge', axis=1)\n","    df_columns_in_common_not_changed['SelectColumnStatement'] = df_columns_in_common_not_changed['COLUMN_NAME']\n","\n","# # select statement generation\n","if not df_columns_only_in_source.empty:\n","    df_columns_only_in_source['SelectColumnStatement'] = \"CAST(NULL AS \" + df_columns_only_in_source['COLUMN_DEFINITION'] + \") AS \" + df_columns_only_in_source['COLUMN_NAME']\n","else:\n","    df_columns_only_in_source = pd.DataFrame()\n","\n","if not df_columns_with_type_changed.empty:\n","    df_columns_with_type_changed['SelectColumnStatement'] = \"CAST([\" + df_columns_with_type_changed['COLUMN_NAME'] + \"] AS \" + df_columns_with_type_changed['COLUMN_DEFINITION'] + \") AS \" + df_columns_with_type_changed['COLUMN_NAME']\n","else:\n","    df_columns_with_type_changed = pd.DataFrame()\n","\n","\n","# sources tables that changed\n","if not df_compare_dataframes_unioned.empty:\n","\n","    # generate a distinct list of tables that changed\n","    df_changed_tables = df_compare_dataframes_unioned[['DATABASE_NAME', 'TABLE_SCHEMA', 'TABLE_NAME']].drop_duplicates()\n","    # build the sql statement to run against the target warehouse\n","    df_sql_statements = pd.concat([df_columns_only_in_source, df_columns_with_type_changed, df_columns_in_common_not_changed]).drop_duplicates().sort_values(by=['DATABASE_NAME', 'TABLE_SCHEMA', 'TABLE_NAME', 'ORDINAL_POSITION' ])\n","    df_sql_statements_grouped = df_sql_statements.groupby(['DATABASE_NAME', 'TABLE_SCHEMA', 'TABLE_NAME'])['SelectColumnStatement'].agg(','.join).reset_index()\n","    df_sql_statements_grouped[\"DropBackupTableStatement\"] = \"DROP TABLE IF EXISTS \" + df_sql_statements_grouped[\"TABLE_SCHEMA\"] + \".\" + df_sql_statements_grouped[\"TABLE_NAME\"] + \"_backup\"\n","    df_sql_statements_grouped[\"CtasStatement\"] = \"CREATE TABLE \" + df_sql_statements_grouped[\"TABLE_SCHEMA\"] + \".\" + df_sql_statements_grouped[\"TABLE_NAME\"] + \"_backup AS SELECT \" + df_sql_statements_grouped[\"SelectColumnStatement\"] + \" FROM \" + df_sql_statements_grouped[\"TABLE_SCHEMA\"] + \".\" + df_sql_statements_grouped[\"TABLE_NAME\"]\n","    df_sql_statements_grouped[\"DropTableStatement\"] = \"DROP TABLE IF EXISTS \" + df_sql_statements_grouped[\"TABLE_SCHEMA\"] + \".\" + df_sql_statements_grouped[\"TABLE_NAME\"]\n","    df_sql_statements_grouped[\"RenamingTableStatement\"] = \"EXEC sp_rename '\" + df_sql_statements_grouped[\"TABLE_SCHEMA\"] + \".\" + df_sql_statements_grouped[\"TABLE_NAME\"] + \"_backup', '\" + df_sql_statements_grouped[\"TABLE_NAME\"] + \"';\"\n","\n","else:\n","    df_changed_tables = pd.DataFrame()\n","\n","\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"89d40c51-79ba-417e-b2a9-8770ce44f46a"},{"cell_type":"markdown","source":["**Run the sql statements against the target sql endpoint**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"01282a03-e82a-459c-b0d1-9619dfce1906"},{"cell_type":"code","source":["\n","try:\n","\n","    # iterate through the source warehouses\n","    for index, row in df_source_warehouses.iterrows():\n","\n","        # get the current warehouse\n","        vWarehouseName = row['Warehouse Name']\n","\n","        if not df_changed_tables.empty:\n","\n","            # filter the changed tables on the current warehouse\n","            df_changed_tables_in_scope = df_changed_tables[df_changed_tables['DATABASE_NAME']==vWarehouseName]\n","\n","            # if the changed tables df is not empty\n","            if not df_changed_tables_in_scope.empty:\n","\n","                # define the connection string for the alchemy engine\n","                vConnectionString = f\"Driver={{ODBC Driver 18 for SQL Server}};Server={vTargetSqlEndpoint};Database={vWarehouseName}\"\n","\n","                # create the sql engine\n","                sql_engine = create_sqlalchemy_engine(vConnectionString)\n","\n","                # connect to the engine\n","                with sql_engine.connect() as sql_connection:\n","\n","                    connection = sql_engine.raw_connection()\n","                    cursor = connection.cursor()\n","\n","\n","                    # iterate over tables that require an update\n","                    for index_table, row_table in df_changed_tables_in_scope.iterrows():\n","                        vChangedSchema = row_table['TABLE_SCHEMA']\n","                        vChangedTable = row_table['TABLE_NAME']\n","\n","                        # filter the sql statements on the current warehouse, schema and table\n","                        df_sql_statements_in_scope = df_sql_statements_grouped[(df_sql_statements_grouped['DATABASE_NAME']==vWarehouseName) & (df_sql_statements_grouped['TABLE_SCHEMA']==vChangedSchema) & (df_sql_statements_grouped['TABLE_NAME']==vChangedTable)]\n","\n","\n","                        # retrieve each of the sql statement and execute it\n","                        vDropBackupTableStatement = df_sql_statements_in_scope.loc[0, 'DropBackupTableStatement'] + ';'\n","                        print(f\"running statement: {vDropBackupTableStatement}\")\n","                        cursor.execute(vDropBackupTableStatement)\n","\n","                        vCtasStatement = df_sql_statements_in_scope.loc[0, 'CtasStatement'] + ';'\n","                        print(f\"running statement: {vCtasStatement}\")\n","                        cursor.execute(vCtasStatement)\n","\n","                        vDropTableStatement = df_sql_statements_in_scope.loc[0, 'DropTableStatement'] + ';'\n","                        print(f\"running statement: {vDropTableStatement}\")\n","                        cursor.execute(vDropTableStatement)\n","\n","                        vRenamingTableStatement = df_sql_statements_in_scope.loc[0, 'RenamingTableStatement'] + ';'\n","                        print(f\"running statement: {vRenamingTableStatement}\")\n","                        cursor.execute(vRenamingTableStatement)\n","\n","                    # commit\n","                    connection.commit()\n","\n","            else:\n","                vMessage = f\"no change detected in warehouse <{vWarehouseName}>\"\n","                if pDebugMode == \"yes\":\n","                    print(vMessage)\n","\n","        else:\n","            vMessage = f\"no change detected in existings warehouses\"\n","            if pDebugMode == \"yes\":\n","                print(vMessage)\n","\n","    # logging\n","    vMessage = f\"succeeded\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'updating target warehouses tables definition', datetime.now(), None, vMessage, ''] \n","\n","except Exception as e:\n","    vMessage = f\"failed\"\n","    dfLogging.loc[len(dfLogging.index)] = [None, vNotebookId, vLogNotebookName, vWorkspaceId, 'updating target warehouses tables definition', datetime.now(), None, vMessage, str(e) ] \n","    if pDebugMode == \"yes\":\n","        print(str(e))\n"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"cc5b6252-376c-42de-a6af-331de022203c"},{"cell_type":"markdown","source":["**Logging**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5c6cbe48-3cd5-4e62-ae79-50f2106e7857"},{"cell_type":"code","source":["try:\n","    # perform the conversion of columns\n","    dfLogging = dfLogging.astype({\n","            \"LoadId\": \"string\",\t\n","            \"NotebookId\": \"string\", \t\n","            \"NotebookName\": \"string\", \n","            \"WorkspaceId\": \"string\", \n","            \"CellId\": \"string\", \n","            \"Timestamp\": \"datetime64[ns]\", \n","            \"ElapsedTime\": \"string\", \n","            \"Message\": \"string\", \n","            \"ErrorMessage\" : \"string\"\n","        })\n","\n","    # save panda dataframe to a spark dataframe \n","    sparkDF_Logging = spark.createDataFrame(dfLogging) \n","\n","    # save to the lakehouse\n","    sparkDF_Logging.write.mode(\"append\").format(\"delta\").option(\"mergeSchema\", \"true\").saveAsTable(\"staging.notebook_logging_cicd\")\n","\n","except Exception as e:\n","    vMessage = \"saving logs to the lakehouse failed\"\n","    if pDebugMode == \"yes\":\n","        print(str(e))"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"b16dcf7c-22b8-4151-aeac-d7423a504fc3"}],"metadata":{"language_info":{"name":"python"},"kernelspec":{"name":"synapse_pyspark","language":"Python","display_name":"Synapse PySpark"},"kernel_info":{"name":"synapse_pyspark"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{},"environment":{}}},"nbformat":4,"nbformat_minor":5}