{"cells":[{"cell_type":"markdown","source":["Mirroring code"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2aaa9dcd-b571-4cfd-b69d-49a698e88b8d"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT License.\n","\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import ClientSecretCredential\n","import requests\n","import json\n","import os\n","\n","class OpenMirroringClient:\n","    def __init__(self, client_id: str, client_secret: str, client_tenant: str, host: str):\n","        self.client_id = client_id\n","        self.client_secret = client_secret\n","        self.client_tenant = client_tenant\n","        self.host = self._normalize_path(host)\n","        self.service_client = self._create_service_client()\n","\n","    def _normalize_path(self, path: str) -> str:\n","        \"\"\"\n","        Normalizes the given path by removing the 'LandingZone' segment if it ends with it.\n","\n","        :param path: The original path.\n","        :return: The normalized path.\n","        \"\"\"\n","        if path.endswith(\"LandingZone\"):\n","            # Remove the 'LandingZone' segment\n","            return path[:path.rfind(\"/LandingZone\")]\n","        elif path.endswith(\"LandingZone/\"):\n","            # Remove the 'LandingZone/' segment\n","            return path[:path.rfind(\"/LandingZone/\")]\n","        return path\n","\n","    def _create_service_client(self):\n","        \"\"\"Creates and returns a DataLakeServiceClient.\"\"\"\n","        try:\n","            credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","            return DataLakeServiceClient(account_url=self.host, credential=credential)\n","        except Exception as e:\n","            raise Exception(f\"Failed to create DataLakeServiceClient: {e}\")\n","\n","    def create_table(self, schema_name: str = None, table_name: str = \"\", key_cols: list = []):\n","        \"\"\"\n","        Creates a folder in OneLake storage and a _metadata.json file inside it.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param key_cols: List of key column names.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Create the folder\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","            directory_client.create_directory()\n","\n","            # Create the _metadata.json file\n","            metadata_content = {\"keyColumns\": [f'{col}' for col in key_cols]}\n","            metadata_file_path = os.path.join(folder_path, \"_metadata.json\")\n","            file_client = directory_client.create_file(\"_metadata.json\")\n","            file_client.append_data(data=json.dumps(metadata_content), offset=0, length=len(json.dumps(metadata_content)))\n","            file_client.flush_data(len(json.dumps(metadata_content)))\n","\n","            print(f\"Folder and _metadata.json created successfully at: {folder_path}\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to create table: {e}\")\n","\n","    def remove_table(self, schema_name: str = None, table_name: str = \"\", remove_schema_folder: bool = False):\n","        \"\"\"\n","        Deletes a folder in the OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param remove_schema_folder: If True, removes the schema folder as well.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                print(f\"Warning: Folder '{folder_path}' not found.\")\n","                return\n","\n","            # Delete the folder\n","            directory_client.delete_directory()\n","            print(f\"Folder '{folder_path}' deleted successfully.\")\n","\n","            # Check if schema folder exists\n","            if remove_schema_folder and schema_name:\n","                schema_folder_path = f\"{schema_name}.schema\"\n","                schema_directory_client = file_system_client.get_directory_client(schema_folder_path)\n","                if schema_directory_client.exists():\n","                    schema_directory_client.delete_directory()\n","                    print(f\"Schema folder '{schema_folder_path}' deleted successfully.\")\n","                else:\n","                    print(f\"Warning: Schema folder '{schema_folder_path}' not found.\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to delete table: {e}\")\n","\n","    def get_next_file_name(self, schema_name: str = None, table_name: str = \"\") -> str:\n","        \"\"\"\n","        Finds the next file name for a folder in OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :return: The next file name padded to 20 digits.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"LandingZone/{schema_name}.schema/{table_name}\" if schema_name else f\"LandingZone/{table_name}\"\n","\n","        try:\n","            # Get the system client\n","            file_system_client = self.service_client.get_file_system_client(file_system=folder_path)\n","\n","            # List all files in the folder\n","            file_list = file_system_client.get_paths(recursive=False)\n","            parquet_files = []\n","\n","            for file in file_list:\n","                file_name = os.path.basename(file.name)\n","                if not file.is_directory and file_name.endswith(\".parquet\") and not file_name.startswith(\"_\"):\n","                    # Validate the file name pattern\n","                    if not file_name[:-8].isdigit() or len(file_name[:-8]) != 20:  # Exclude \".parquet\"\n","                        raise ValueError(f\"Invalid file name pattern: {file_name}\")\n","                    parquet_files.append(int(file_name[:-8]))\n","\n","            # Determine the next file name\n","            if parquet_files:\n","                next_file_number = max(parquet_files) + 1\n","            else:\n","                next_file_number = 1\n","\n","            # Return the next file name padded to 20 digits\n","            return f\"{next_file_number:020}.parquet\"\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to get next file name: {e}\")\n","\n","    def upload_data_file(self, schema_name: str = None, table_name: str = \"\", local_file_path: str = \"\"):\n","        \"\"\"\n","        Uploads a file to OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param local_file_path: Path to the local file to be uploaded.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","        if not local_file_path or not os.path.isfile(local_file_path):\n","            raise ValueError(\"Invalid local file path.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                raise FileNotFoundError(f\"Folder '{folder_path}' not found.\")\n","\n","            # Get the next file name\n","            next_file_name = self.get_next_file_name(schema_name, table_name)\n","\n","            # Add an underscore to the file name for temporary upload\n","            temp_file_name = f\"_{next_file_name}\"\n","\n","            # Upload the file\n","            file_client = directory_client.create_file(temp_file_name)\n","            with open(local_file_path, \"rb\") as file_data:\n","                file_contents = file_data.read()\n","                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n","                file_client.flush_data(len(file_contents))\n","\n","            print(f\"File uploaded successfully as '{temp_file_name}'.\")\n","            \n","            # Python SDK doesn't handle rename properly for onelake, using REST API to rename the file instead\n","            self.rename_file_via_rest_api(f\"LandingZone/{folder_path}\", temp_file_name, next_file_name)\n","            print(f\"File renamed successfully to '{next_file_name}'.\")\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to upload data file: {e}\")\n","        \n","    def rename_file_via_rest_api(self, folder_path: str, old_file_name: str, new_file_name: str):\n","        # Create a ClientSecretCredential\n","        credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","        # Get a token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        # Construct the rename URL\n","        rename_url = f\"{self.host}/{folder_path}/{new_file_name}\"\n","\n","        # Construct the source path\n","        source_path = f\"{self.host}/{folder_path}/{old_file_name}\"\n","\n","        # Set the headers\n","        headers = {\n","            \"Authorization\": f\"Bearer {token}\",\n","            \"x-ms-rename-source\": source_path,\n","            \"x-ms-version\": \"2020-06-12\"\n","        }\n","\n","        # Send the rename request\n","        response = requests.put(rename_url, headers=headers)\n","\n","        if response.status_code in [200, 201]:\n","            print(f\"File renamed from {old_file_name} to {new_file_name} successfully.\")\n","        else:\n","            print(f\"Failed to rename file. Status code: {response.status_code}, Error: {response.text}\")\n","\n","    def get_mirrored_database_status(self):\n","        \"\"\"\n","        Retrieves and displays the status of the mirrored database from Monitoring/replicator.json.\n","\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"replicator.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","            print(json.dumps(status_json, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","    def get_table_status(self, schema_name: str = None, table_name: str = None):\n","        \"\"\"\n","        Retrieves and displays the status of tables from Monitoring/table.json.\n","\n","        :param schema_name: Optional schema name to filter.\n","        :param table_name: Optional table name to filter.\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"tables.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","\n","            # Treat None as empty string for filtering\n","            schema_name = schema_name or \"\"\n","            table_name = table_name or \"\"\n","\n","            if not schema_name and not table_name:\n","                # Show the whole JSON content\n","                print(json.dumps(status_json, indent=4))\n","            else:\n","                # Filter tables array\n","                filtered_tables = [\n","                    t for t in status_json.get(\"tables\", [])\n","                    if t.get(\"sourceSchemaName\", \"\") == schema_name and t.get(\"sourceTableName\", \"\") == table_name\n","                ]\n","                print(json.dumps({\"tables\": filtered_tables}, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"b2cd9b39-28fc-4380-b570-0fae8d6bfa22","normalized_state":"finished","queued_time":"2025-07-25T14:23:30.9576659Z","session_start_time":null,"execution_start_time":"2025-07-25T14:23:30.958645Z","execution_finish_time":"2025-07-25T14:23:32.7741842Z","parent_msg_id":"d3283974-234a-4af2-bacf-572c6be9a7a4"}},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f2678756-f614-42d0-b4b9-3354ca7ab0de"},{"cell_type":"markdown","source":["Code to connect to sharepoint and extract a list"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"c564efea-1f8c-4580-9ae0-6c1aac6f4237"},{"cell_type":"code","source":["import requests\n","import pandas as pd\n","\n","def mirror_sharepoint_list_to_parquet(\n","    tenant_id: str,\n","    client_id: str,\n","    client_secret: str,\n","    sharepoint_domain: str,\n","    site_name: str,\n","    list_name: str,\n","    selected_columns: list,\n","    output_parquet_path: str\n","):\n","    \"\"\"\n","    Fetches SharePoint list data and writes selected columns to a Parquet file in the Lakehouse.\n","\n","    Args:\n","        tenant_id (str): Azure AD tenant ID.\n","        client_id (str): Azure AD application (client) ID.\n","        client_secret (str): Azure AD client secret.\n","        sharepoint_domain (str): SharePoint domain (e.g., 'contoso.sharepoint.com').\n","        site_name (str): SharePoint site name (e.g., 'mysite').\n","        list_name (str): Name of the SharePoint list.\n","        selected_columns (list): List of fields to extract from SharePoint.\n","        output_parquet_path (str): Full path where the Parquet file should be written (e.g., '/lakehouse/default/Files/mylist.parquet').\n","    \"\"\"\n","\n","    # 1. Get Azure AD token\n","    token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n","    token_data = {\n","        'grant_type': 'client_credentials',\n","        'client_id': client_id,\n","        'client_secret': client_secret,\n","        'scope': 'https://graph.microsoft.com/.default'\n","    }\n","    token_resp = requests.post(token_url, data=token_data)\n","    token_resp.raise_for_status()\n","    access_token = token_resp.json()['access_token']\n","\n","    headers = {\n","        'Authorization': f'Bearer {access_token}',\n","        'Accept': 'application/json'\n","    }\n","\n","    # 2. Get Site ID\n","    site_url = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_domain}:/sites/{site_name}\"\n","    site_response = requests.get(site_url, headers=headers)\n","    site_response.raise_for_status()\n","    site_id = site_response.json()['id']\n","\n","    # 3. Get List Items\n","    list_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/lists/{list_name}/items?expand=fields\"\n","    list_resp = requests.get(list_url, headers=headers)\n","    list_resp.raise_for_status()\n","    items = list_resp.json().get('value', [])\n","    fields_data = [item['fields'] for item in items]\n","\n","    # 4. Convert to DataFrame\n","    df = pd.DataFrame(fields_data)\n","\n","    # 5. Select specified columns if present\n","    df_selected = df[selected_columns] if selected_columns else df\n","    df_selected['__rowMarker__'] = 1\n","\n","    # 6. Save to Parquet\n","    df_selected.to_parquet(output_parquet_path, index=False)\n","\n","    print(f\"Data from SharePoint list '{list_name}' written to {output_parquet_path}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"b2cd9b39-28fc-4380-b570-0fae8d6bfa22","normalized_state":"finished","queued_time":"2025-07-25T14:47:49.6548988Z","session_start_time":null,"execution_start_time":"2025-07-25T14:47:49.6558455Z","execution_finish_time":"2025-07-25T14:47:49.9722482Z","parent_msg_id":"fba43b1e-50d2-4d3c-9025-fee158b98168"}},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2440e6b3-96ba-4a0e-8c7a-11fb494a73d9"},{"cell_type":"markdown","source":["# Variables\n","\n","Replace the variables with the ones from your system"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f3d2e696-d856-44ff-9253-8f92b5d9ed68"},{"cell_type":"code","source":["tenant_id=\"{entra_tenant_id}\"\n","client_id=\"{service_principal_id}\"\n","client_secret=\"{service_principal_secret}\"\n","\n","sharepoint_domain=\"{sharepoint_domain}.sharepoint.com\"\n","site_name=\"{sharepoint_site_name}\"\n","list_name=\"{sharepoint_list_name}\"\n","selected_columns=['Name', 'type', 'species', 'legs', 'id', 'Modified', 'Created'] #replace with columns if you want\n","output_parquet_path=\"/lakehouse/default/Files/{anything you like}.parquet\"\n","\n","mirroring_schema = \"{mirroring_schema}\"\n","mirroring_table = \"{mirroring_table}\"\n","mirroring_key_col = [\"id\"]\n","\n","landing_zone = \"https://onelake.dfs.fabric.microsoft.com/{workspaceid}/{mirrored database id}/Files/LandingZone\"\n","\n","client = OpenMirroringClient(\n","    client_id=client_id,\n","    client_secret=client_secret,\n","    client_tenant=tenant_id,\n","    host=landing_zone\n",")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"b2cd9b39-28fc-4380-b570-0fae8d6bfa22","normalized_state":"finished","queued_time":"2025-07-25T14:47:53.9704763Z","session_start_time":null,"execution_start_time":"2025-07-25T14:47:53.9714313Z","execution_finish_time":"2025-07-25T14:47:54.2938326Z","parent_msg_id":"1784b8f3-c751-4b29-93d1-39a62ac6db98"}},"metadata":{}}],"execution_count":13,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"a7afc26e-91de-4093-b7ef-dd9e2defd1fc"},{"cell_type":"code","source":["# client.remove_table(schema_name=mirroring_schema, table_name=mirroring_table)\n","\n","# Create a table, only do the first time\n","client.create_table(schema_name=mirroring_schema, table_name=mirroring_table, key_cols=mirroring_key_col )\n","\n","# export sharepoint list to file\n","mirror_sharepoint_list_to_parquet(tenant_id,client_id,client_secret,sharepoint_domain,site_name,list_name,selected_columns,output_parquet_path)\n","# Upload a file\n","client.upload_data_file(schema_name=mirroring_schema, table_name=mirroring_table, local_file_path=output_parquet_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"b2cd9b39-28fc-4380-b570-0fae8d6bfa22","normalized_state":"finished","queued_time":"2025-07-25T14:48:25.2938727Z","session_start_time":null,"execution_start_time":"2025-07-25T14:48:25.2948072Z","execution_finish_time":"2025-07-25T14:48:28.3552172Z","parent_msg_id":"1118e082-7f96-49fd-83fc-dcf188df8bfc"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Folder 'sharepoint.schema/friends' deleted successfully.\nFolder and _metadata.json created successfully at: sharepoint.schema/friends\nData from SharePoint list 'friends' written to /lakehouse/default/Files/friends.parquet\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\n"]}],"execution_count":14,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"1b01a641-16d3-4181-a4d1-5bb914562d95"},{"cell_type":"code","source":["# run just this on a schedule to move changes\n","# export sharepoint list to file\n","mirror_sharepoint_list_to_parquet(tenant_id,client_id,client_secret,sharepoint_domain,site_name,list_name,selected_columns,output_parquet_path)\n","# Upload a file\n","client.upload_data_file(schema_name=mirroring_schema, table_name=mirroring_table, local_file_path=output_parquet_path)\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"b2cd9b39-28fc-4380-b570-0fae8d6bfa22","normalized_state":"finished","queued_time":"2025-07-25T14:50:57.3186848Z","session_start_time":null,"execution_start_time":"2025-07-25T14:50:57.3198191Z","execution_finish_time":"2025-07-25T14:50:59.4110017Z","parent_msg_id":"bde8ed5b-6719-4912-bbd9-ba758c31f0d9"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Data from SharePoint list 'friends' written to /lakehouse/default/Files/friends.parquet\nFile uploaded successfully as '_00000000000000000003.parquet'.\nFile renamed from _00000000000000000003.parquet to 00000000000000000003.parquet successfully.\nFile renamed successfully to '00000000000000000003.parquet'.\n"]}],"execution_count":16,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"721b15b7-a28b-4463-8c2f-8a6950cfe1c8"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"70927777-be68-4f8f-a1c0-33c98979aeac","known_lakehouses":[{"id":"70927777-be68-4f8f-a1c0-33c98979aeac"}],"default_lakehouse_name":"mirroringlake","default_lakehouse_workspace_id":"e3b15d2f-50e5-4f8c-9d09-c57164f31268"}}},"nbformat":4,"nbformat_minor":5}