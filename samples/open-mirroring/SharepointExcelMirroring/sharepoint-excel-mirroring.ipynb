{"cells":[{"cell_type":"markdown","source":["Mirroring code"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"2aaa9dcd-b571-4cfd-b69d-49a698e88b8d"},{"cell_type":"code","source":["# Copyright (c) Microsoft Corporation. All rights reserved.\n","# Licensed under the MIT License.\n","\n","from azure.storage.filedatalake import DataLakeServiceClient\n","from azure.identity import ClientSecretCredential\n","import requests\n","import json\n","import os\n","\n","class OpenMirroringClient:\n","    def __init__(self, client_id: str, client_secret: str, client_tenant: str, host: str):\n","        self.client_id = client_id\n","        self.client_secret = client_secret\n","        self.client_tenant = client_tenant\n","        self.host = self._normalize_path(host)\n","        self.service_client = self._create_service_client()\n","\n","    def _normalize_path(self, path: str) -> str:\n","        \"\"\"\n","        Normalizes the given path by removing the 'LandingZone' segment if it ends with it.\n","\n","        :param path: The original path.\n","        :return: The normalized path.\n","        \"\"\"\n","        if path.endswith(\"LandingZone\"):\n","            # Remove the 'LandingZone' segment\n","            return path[:path.rfind(\"/LandingZone\")]\n","        elif path.endswith(\"LandingZone/\"):\n","            # Remove the 'LandingZone/' segment\n","            return path[:path.rfind(\"/LandingZone/\")]\n","        return path\n","\n","    def _create_service_client(self):\n","        \"\"\"Creates and returns a DataLakeServiceClient.\"\"\"\n","        try:\n","            credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","            return DataLakeServiceClient(account_url=self.host, credential=credential)\n","        except Exception as e:\n","            raise Exception(f\"Failed to create DataLakeServiceClient: {e}\")\n","\n","    def create_table(self, schema_name: str = None, table_name: str = \"\", key_cols: list = []):\n","        \"\"\"\n","        Creates a folder in OneLake storage and a _metadata.json file inside it.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param key_cols: List of key column names.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Create the folder\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","            directory_client.create_directory()\n","\n","            # Create the _metadata.json file\n","            metadata_content = {\"keyColumns\": [f'{col}' for col in key_cols]}\n","            metadata_file_path = os.path.join(folder_path, \"_metadata.json\")\n","            file_client = directory_client.create_file(\"_metadata.json\")\n","            file_client.append_data(data=json.dumps(metadata_content), offset=0, length=len(json.dumps(metadata_content)))\n","            file_client.flush_data(len(json.dumps(metadata_content)))\n","\n","            print(f\"Folder and _metadata.json created successfully at: {folder_path}\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to create table: {e}\")\n","\n","    def remove_table(self, schema_name: str = None, table_name: str = \"\", remove_schema_folder: bool = False):\n","        \"\"\"\n","        Deletes a folder in the OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param remove_schema_folder: If True, removes the schema folder as well.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                print(f\"Warning: Folder '{folder_path}' not found.\")\n","                return\n","\n","            # Delete the folder\n","            directory_client.delete_directory()\n","            print(f\"Folder '{folder_path}' deleted successfully.\")\n","\n","            # Check if schema folder exists\n","            if remove_schema_folder and schema_name:\n","                schema_folder_path = f\"{schema_name}.schema\"\n","                schema_directory_client = file_system_client.get_directory_client(schema_folder_path)\n","                if schema_directory_client.exists():\n","                    schema_directory_client.delete_directory()\n","                    print(f\"Schema folder '{schema_folder_path}' deleted successfully.\")\n","                else:\n","                    print(f\"Warning: Schema folder '{schema_folder_path}' not found.\")\n","        except Exception as e:\n","            raise Exception(f\"Failed to delete table: {e}\")\n","\n","    def get_next_file_name(self, schema_name: str = None, table_name: str = \"\") -> str:\n","        \"\"\"\n","        Finds the next file name for a folder in OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :return: The next file name padded to 20 digits.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"LandingZone/{schema_name}.schema/{table_name}\" if schema_name else f\"LandingZone/{table_name}\"\n","\n","        try:\n","            # Get the system client\n","            file_system_client = self.service_client.get_file_system_client(file_system=folder_path)\n","\n","            # List all files in the folder\n","            file_list = file_system_client.get_paths(recursive=False)\n","            parquet_files = []\n","\n","            for file in file_list:\n","                file_name = os.path.basename(file.name)\n","                if not file.is_directory and file_name.endswith(\".parquet\") and not file_name.startswith(\"_\"):\n","                    # Validate the file name pattern\n","                    if not file_name[:-8].isdigit() or len(file_name[:-8]) != 20:  # Exclude \".parquet\"\n","                        raise ValueError(f\"Invalid file name pattern: {file_name}\")\n","                    parquet_files.append(int(file_name[:-8]))\n","\n","            # Determine the next file name\n","            if parquet_files:\n","                next_file_number = max(parquet_files) + 1\n","            else:\n","                next_file_number = 1\n","\n","            # Return the next file name padded to 20 digits\n","            return f\"{next_file_number:020}.parquet\"\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to get next file name: {e}\")\n","\n","    def upload_data_file(self, schema_name: str = None, table_name: str = \"\", local_file_path: str = \"\"):\n","        \"\"\"\n","        Uploads a file to OneLake storage.\n","\n","        :param schema_name: Optional schema name.\n","        :param table_name: Name of the table.\n","        :param local_file_path: Path to the local file to be uploaded.\n","        \"\"\"\n","        if not table_name:\n","            raise ValueError(\"table_name cannot be empty.\")\n","        if not local_file_path or not os.path.isfile(local_file_path):\n","            raise ValueError(\"Invalid local file path.\")\n","\n","        # Construct the folder path\n","        folder_path = f\"{schema_name}.schema/{table_name}\" if schema_name else f\"{table_name}\"\n","\n","        try:\n","            # Get the directory client\n","            file_system_client = self.service_client.get_file_system_client(file_system=\"LandingZone\")  # Replace with your file system name\n","            directory_client = file_system_client.get_directory_client(folder_path)\n","\n","            # Check if the folder exists\n","            if not directory_client.exists():\n","                raise FileNotFoundError(f\"Folder '{folder_path}' not found.\")\n","\n","            # Get the next file name\n","            next_file_name = self.get_next_file_name(schema_name, table_name)\n","\n","            # Add an underscore to the file name for temporary upload\n","            temp_file_name = f\"_{next_file_name}\"\n","\n","            # Upload the file\n","            file_client = directory_client.create_file(temp_file_name)\n","            with open(local_file_path, \"rb\") as file_data:\n","                file_contents = file_data.read()\n","                file_client.append_data(data=file_contents, offset=0, length=len(file_contents))\n","                file_client.flush_data(len(file_contents))\n","\n","            print(f\"File uploaded successfully as '{temp_file_name}'.\")\n","            \n","            # Python SDK doesn't handle rename properly for onelake, using REST API to rename the file instead\n","            self.rename_file_via_rest_api(f\"LandingZone/{folder_path}\", temp_file_name, next_file_name)\n","            print(f\"File renamed successfully to '{next_file_name}'.\")\n","\n","        except Exception as e:\n","            raise Exception(f\"Failed to upload data file: {e}\")\n","        \n","    def rename_file_via_rest_api(self, folder_path: str, old_file_name: str, new_file_name: str):\n","        # Create a ClientSecretCredential\n","        credential = ClientSecretCredential(self.client_tenant, self.client_id, self.client_secret)            \n","        # Get a token\n","        token = credential.get_token(\"https://storage.azure.com/.default\").token\n","\n","        # Construct the rename URL\n","        rename_url = f\"{self.host}/{folder_path}/{new_file_name}\"\n","\n","        # Construct the source path\n","        source_path = f\"{self.host}/{folder_path}/{old_file_name}\"\n","\n","        # Set the headers\n","        headers = {\n","            \"Authorization\": f\"Bearer {token}\",\n","            \"x-ms-rename-source\": source_path,\n","            \"x-ms-version\": \"2020-06-12\"\n","        }\n","\n","        # Send the rename request\n","        response = requests.put(rename_url, headers=headers)\n","\n","        if response.status_code in [200, 201]:\n","            print(f\"File renamed from {old_file_name} to {new_file_name} successfully.\")\n","        else:\n","            print(f\"Failed to rename file. Status code: {response.status_code}, Error: {response.text}\")\n","\n","    def get_mirrored_database_status(self):\n","        \"\"\"\n","        Retrieves and displays the status of the mirrored database from Monitoring/replicator.json.\n","\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"replicator.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","            print(json.dumps(status_json, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","    def get_table_status(self, schema_name: str = None, table_name: str = None):\n","        \"\"\"\n","        Retrieves and displays the status of tables from Monitoring/table.json.\n","\n","        :param schema_name: Optional schema name to filter.\n","        :param table_name: Optional table name to filter.\n","        :raises Exception: If the status file or path does not exist.\n","        \"\"\"\n","        file_system_client = self.service_client.get_file_system_client(file_system=\"Monitoring\")\n","        try:\n","            file_client = file_system_client.get_file_client(\"tables.json\")\n","            if not file_client.exists():\n","                raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")\n","\n","            download = file_client.download_file()\n","            content = download.readall()\n","            status_json = json.loads(content)\n","\n","            # Treat None as empty string for filtering\n","            schema_name = schema_name or \"\"\n","            table_name = table_name or \"\"\n","\n","            if not schema_name and not table_name:\n","                # Show the whole JSON content\n","                print(json.dumps(status_json, indent=4))\n","            else:\n","                # Filter tables array\n","                filtered_tables = [\n","                    t for t in status_json.get(\"tables\", [])\n","                    if t.get(\"sourceSchemaName\", \"\") == schema_name and t.get(\"sourceTableName\", \"\") == table_name\n","                ]\n","                print(json.dumps({\"tables\": filtered_tables}, indent=4))\n","        except Exception:\n","            raise Exception(\"No status of mirrored database has been found. Please check whether the mirrored database has been started properly.\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"f4c95229-265d-42c6-8947-6fd61adda3b2","normalized_state":"finished","queued_time":"2025-07-25T16:11:52.0872446Z","session_start_time":null,"execution_start_time":"2025-07-25T16:11:52.0883273Z","execution_finish_time":"2025-07-25T16:11:52.4255283Z","parent_msg_id":"c598f30d-d9ca-4923-ba6d-9853a6ebedc6"}},"metadata":{}}],"execution_count":52,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f2678756-f614-42d0-b4b9-3354ca7ab0de"},{"cell_type":"markdown","source":["Code to connect to sharepoint and extract a list"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"c564efea-1f8c-4580-9ae0-6c1aac6f4237"},{"cell_type":"markdown","source":["Variables"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"f3d2e696-d856-44ff-9253-8f92b5d9ed68"},{"cell_type":"code","source":["import requests\n","import os\n","\n","def download_excel_files_from_sharepoint_folder(\n","    tenant_id: str,\n","    client_id: str,\n","    client_secret: str,\n","    sharepoint_domain: str,\n","    site_name: str,\n","    folder_name: str,\n","    lakehouse_subfolder: str = \"excelfiles\"\n","):\n","    \"\"\"\n","    Downloads all .xlsx files from a specified SharePoint folder and saves them to a OneLake location.\n","    \n","    Args:\n","        tenant_id (str): Azure AD tenant ID.\n","        client_id (str): App registration client ID.\n","        client_secret (str): App registration client secret.\n","        sharepoint_domain (str): e.g. 'contoso.sharepoint.com'\n","        site_name (str): e.g. 'demosite'\n","        folder_name (str): The name of the subfolder (e.g. 'exceldocs') inside Shared Documents\n","        lakehouse_subfolder (str): OneLake path to save the Excel files (under /lakehouse/default/Files)\n","    \"\"\"\n","\n","    # Create download directory if it doesn't exist\n","    download_dir = f\"/lakehouse/default/Files/{lakehouse_subfolder}\"\n","    os.makedirs(download_dir, exist_ok=True)\n","\n","    # === 1. Get Access Token ===\n","    token_url = f\"https://login.microsoftonline.com/{tenant_id}/oauth2/v2.0/token\"\n","    token_data = {\n","        'grant_type': 'client_credentials',\n","        'client_id': client_id,\n","        'client_secret': client_secret,\n","        'scope': 'https://graph.microsoft.com/.default'\n","    }\n","    access_token = requests.post(token_url, data=token_data).json()['access_token']\n","    headers = {\n","        'Authorization': f'Bearer {access_token}',\n","        'Accept': 'application/json'\n","    }\n","\n","    # === 2. Get Site ID ===\n","    site_url = f\"https://graph.microsoft.com/v1.0/sites/{sharepoint_domain}:/sites/{site_name}\"\n","    site_id = requests.get(site_url, headers=headers).json()['id']\n","\n","    # === 3. Get contents of the target folder ===\n","    folder_path = f\"/{folder_name}\"\n","    list_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/root:{folder_path}:/children\"\n","    response = requests.get(list_url, headers=headers)\n","    response.raise_for_status()\n","\n","    items = response.json().get('value', [])\n","\n","    print(f\"\\n📂 Contents of '{folder_name}' folder:\\n\")\n","    for item in items:\n","        icon = \"📁\" if \"folder\" in item else \"📄\"\n","        print(f\"{icon} {item['name']}\")\n","\n","    # === 4. Download Excel files ===\n","    for item in items:\n","        if \"file\" in item and item[\"name\"].endswith(\".xlsx\"):\n","            file_name = item[\"name\"]\n","            file_id = item[\"id\"]\n","\n","            # Download URL\n","            download_url = f\"https://graph.microsoft.com/v1.0/sites/{site_id}/drive/items/{file_id}/content\"\n","            file_response = requests.get(download_url, headers=headers)\n","            file_response.raise_for_status()\n","\n","            save_path = os.path.join(download_dir, file_name)\n","            with open(save_path, \"wb\") as f:\n","                f.write(file_response.content)\n","\n","            print(f\"✅ Downloaded: {file_name} → {save_path}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"f4c95229-265d-42c6-8947-6fd61adda3b2","normalized_state":"finished","queued_time":"2025-07-25T16:11:52.2041852Z","session_start_time":null,"execution_start_time":"2025-07-25T16:11:52.4267909Z","execution_finish_time":"2025-07-25T16:11:52.7442978Z","parent_msg_id":"d60a668e-1ea0-4096-9a5f-8aea6ebab385"}},"metadata":{}}],"execution_count":53,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"7cd5edd3-96a3-406e-8609-b90002f96eb5"},{"cell_type":"code","source":["import pandas as pd\n","import os\n","def Mirror_Excel_File(folder_path,  tmp_location, clean):\n","    try:\n","        # folder_path - where the Excel files are\n","        # tmp_location - temp storage\n","        # clean - if true, we delete the tables from the mirrored database first\n","        # Read the Excel file\n","        for filename in os.listdir(folder_path):\n","            if filename.endswith('.xlsx'):  \n","                excel_file = os.path.join(folder_path, filename)\n","                file_base = os.path.splitext(os.path.basename(excel_file))[0]\n","                xls = pd.ExcelFile(excel_file)\n","                sheet_names = xls.sheet_names\n","                # Convert each sheet to a separate CSV\n","                for sheet in sheet_names:\n","                    if clean == \"true\" :\n","                        client.remove_table(schema_name=file_base, table_name=sheet)\n","                    client.create_table(schema_name=file_base, table_name=sheet, key_cols=[\"__rowid__\"])\n","                    df = pd.read_excel(xls, sheet_name=sheet)\n","                    newpath = f\"{tmp_location}mirroring/{file_base}.schema/\"\n","                    csv_file = f\"{newpath}{sheet}.parquet\"\n","                    df['__rowMarker__'] = '1'\n","                    df['__rowid__'] = range(1, len(df) + 1)\n","                    if not os.path.exists(newpath):\n","                        os.makedirs(newpath)\n","                    if os.path.exists(csv_file):\n","                        os.remove(csv_file)\n","                    df.to_parquet(csv_file, index=False, )\n","                    print(f\"Saved '{sheet}' to '{csv_file}'\")\n","                    client.upload_data_file(schema_name=file_base, table_name=sheet, local_file_path=csv_file)  \n","                    print(f\"Uploaded schema:'{file_base}' to tablename:'{sheet}'\")\n","    except Exception as e:\n","        print(f\"An error occurred: {e}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"f4c95229-265d-42c6-8947-6fd61adda3b2","normalized_state":"finished","queued_time":"2025-07-25T16:22:27.7415024Z","session_start_time":null,"execution_start_time":"2025-07-25T16:22:27.7423977Z","execution_finish_time":"2025-07-25T16:22:28.074373Z","parent_msg_id":"0b41f653-aca2-44a6-a6a7-26429883fc38"}},"metadata":{}}],"execution_count":62,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"158932d3-10d9-4e0c-8e11-fe3b08d87026"},{"cell_type":"code","source":["tenant_id=\"{entra_tenant_id}\"\n","client_id=\"{service_principal_id}\"\n","client_secret=\"{service_principal_secret}\"\n","\n","sharepoint_domain=\"{sharepoint_domain}.sharepoint.com\"\n","site_name=\"{sharepoint_site_name}\"\n","sharepoint_folder = \"exceldocs\"  # The subfolder you want to inspect\n","lakehouse_subfolder=\"excelfiles\" # Where its going to store th files\n","\n","excel_path = f\"/lakehouse/default/Files/{lakehouse_subfolder}/\"  # Replace with your Excel file name\n","temp_file = \"/lakehouse/default/Files/temp/\"      # Replace with your desired temp location file name\n","\n","landing_zone = \"https://onelake.dfs.fabric.microsoft.com/{workspaceid}/{mirrored database id}/Files/LandingZone\"\n","\n","# Setup the OpenMirroring Client\n","client = OpenMirroringClient(\n","    client_id=client_id,\n","    client_secret=client_secret,\n","    client_tenant=tenant_id,\n","    host=landing_zone\n",")\n","\n","\n","download_excel_files_from_sharepoint_folder(\n","    tenant_id,\n","    client_id,\n","    client_secret,\n","    sharepoint_domain,\n","    site_name,\n","    sharepoint_folder,\n","    lakehouse_subfolder\n",")\n","\n","\n","# Do this for the first time you run it, or you want to reset mirroring\n","#Mirror_Excel_File( excel_path,  temp_file ,\"true\")\n","\n","# do this for the 2nd / 3rd runs.\n","Mirror_Excel_File( excel_path,  temp_file ,\"false\")\n","\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.jupyter.statement-meta+json":{"session_id":"f4c95229-265d-42c6-8947-6fd61adda3b2","normalized_state":"finished","queued_time":"2025-07-25T16:22:36.6992356Z","session_start_time":null,"execution_start_time":"2025-07-25T16:22:36.7002145Z","execution_finish_time":"2025-07-25T16:22:42.3716904Z","parent_msg_id":"e467c13c-67a1-4c6f-8f06-0cf6fc8b9476"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["\n📂 Contents of 'exceldocs' folder:\n\n📄 Book2.xlsx\n✅ Downloaded: Book2.xlsx → /lakehouse/default/Files/excelfiles/Book2.xlsx\nFolder and _metadata.json created successfully at: Book.schema/demo\nSaved 'demo' to '/lakehouse/default/Files/temp/mirroring/Book.schema/demo.parquet'\nFile uploaded successfully as '_00000000000000000004.parquet'.\nFile renamed from _00000000000000000004.parquet to 00000000000000000004.parquet successfully.\nFile renamed successfully to '00000000000000000004.parquet'.\nUploaded schema:'Book' to tablename:'demo'\nFolder and _metadata.json created successfully at: Book.schema/friends\nSaved 'friends' to '/lakehouse/default/Files/temp/mirroring/Book.schema/friends.parquet'\nFile uploaded successfully as '_00000000000000000004.parquet'.\nFile renamed from _00000000000000000004.parquet to 00000000000000000004.parquet successfully.\nFile renamed successfully to '00000000000000000004.parquet'.\nUploaded schema:'Book' to tablename:'friends'\nFolder and _metadata.json created successfully at: Book2.schema/demo\nSaved 'demo' to '/lakehouse/default/Files/temp/mirroring/Book2.schema/demo.parquet'\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nUploaded schema:'Book2' to tablename:'demo'\nFolder and _metadata.json created successfully at: Book2.schema/friends\nSaved 'friends' to '/lakehouse/default/Files/temp/mirroring/Book2.schema/friends.parquet'\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nUploaded schema:'Book2' to tablename:'friends'\nFolder and _metadata.json created successfully at: Book2.schema/Sheet1\nSaved 'Sheet1' to '/lakehouse/default/Files/temp/mirroring/Book2.schema/Sheet1.parquet'\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nUploaded schema:'Book2' to tablename:'Sheet1'\nFolder and _metadata.json created successfully at: Book2.schema/Sheet2\nSaved 'Sheet2' to '/lakehouse/default/Files/temp/mirroring/Book2.schema/Sheet2.parquet'\nFile uploaded successfully as '_00000000000000000001.parquet'.\nFile renamed from _00000000000000000001.parquet to 00000000000000000001.parquet successfully.\nFile renamed successfully to '00000000000000000001.parquet'.\nUploaded schema:'Book2' to tablename:'Sheet2'\n"]}],"execution_count":63,"metadata":{"microsoft":{"language":"python","language_group":"jupyter_python"}},"id":"522517b7-400e-4105-a27f-93c54ee1f54d"}],"metadata":{"kernel_info":{"name":"jupyter","jupyter_kernel_name":"python3.11"},"kernelspec":{"name":"jupyter","display_name":"Jupyter"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"jupyter_python","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"70927777-be68-4f8f-a1c0-33c98979aeac","known_lakehouses":[{"id":"70927777-be68-4f8f-a1c0-33c98979aeac"}],"default_lakehouse_name":"mirroringlake","default_lakehouse_workspace_id":"e3b15d2f-50e5-4f8c-9d09-c57164f31268"}}},"nbformat":4,"nbformat_minor":5}