{"cells":[{"cell_type":"code","source":["import json \n","import time \n","import struct \n","import sqlalchemy \n","import pyodbc \n","import notebookutils \n","import pandas as pd \n","from pyspark.sql import functions as fn \n","from datetime import datetime \n","import sempy.fabric as fabric \n","from sempy.fabric.exceptions import FabricHTTPException, WorkspaceNotFoundException \n","\n","def pad_or_truncate_string(input_string, length, pad_char=' '):\n","    # Truncate if the string is longer than the specified length\n","    if len(input_string) > length:\n","        return input_string[:length]\n","    # Pad if the string is shorter than the specified length\n","    return input_string.ljust(length, pad_char)\n","\n","def display_return(data):\n","    table_details = [\n","        {\n","        'tableName': table['tableName'],\n","        'status':  table['status'],\n","        'startDateTime':  table['startDateTime'],\n","        'endDateTime':  table['endDateTime'],\n","        'lastSuccessfulSyncDateTime':  table['lastSuccessfulSyncDateTime'],\n","        'error':  table['error']\n","        }\n","        for table in data\n","        ]\n","    for detail in table_details:\n","        print(f\"Table: {pad_or_truncate_string(detail['tableName'],20)} status: {detail['status']} start: {detail['startDateTime']}  end: {detail['endDateTime']}  Last Update: {detail['lastSuccessfulSyncDateTime']}  error: {detail['error']}  \")\n","    print('')\n","    #print(data)\n","    \n","    return;\n","\n","tenant_id=spark.conf.get(\"trident.tenant.id\")\n","workspace_id=spark.conf.get(\"trident.workspace.id\")\n","lakehouse_id=spark.conf.get(\"trident.lakehouse.id\")\n","lakehouse_name=spark.conf.get(\"trident.lakehouse.name\")\n","\n","#Instantiate the client\n","client = fabric.FabricRestClient()\n","\n","# This is the SQL endpoint I want to sync with the lakehouse, this needs to be the GUI\n","sqlendpoint = fabric.FabricRestClient().get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()['properties']['sqlEndpointProperties']['id']\n","j_son = fabric.FabricRestClient().get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()\n","display(sqlendpoint)\n"," \n","\n","# URI for the call \n","uri = f\"v1/workspaces/{workspace_id}/sqlEndpoints/{sqlendpoint}/refreshMetadata\" \n","\n","# This is the action, we want to take \n","payload = {} \n","# Payload for a specific table\n","#payload = { \"tableDefinitions\": [  {\"schema\": \"dbo\", \"tableNames\": [\"demotable\"]} ] } \n","\n","# Call the REST API \n","display(f\"uri: {uri}\") \n","\n","try:\n","    response = client.post(uri,json= payload) \n","\n","    ## You should add some error handling here \n","\n","    match response.status_code: \n","\n","        case 200: # The request completed in under 30 seconds so it's treated as synchronous. \n","\n","            data = json.loads(response.text) \n","            display(f\"200:\") \n","            display_return(data)\n","            display(response.text)\n","\n","        case 202: # The request took longer than 30 seconds so it's treated as asynchronous. \n","\n","            operation_id = response.headers[\"x-ms-operation-id\"] \n","            retry_delay = int(response.headers[\"retry-after\"] )\n","            lro_uri = f\"v1/operations/{operation_id}\" \n","            lro_result_uri = f\"{lro_uri}/result\" \n","        \n","            display(f\"202:\") \n","                \n","            #display(f\"retry_delay:{retry_delay}\")\n","\n","            while True: \n","\n","                lro_response = client.get(lro_uri) \n","                lro_data = json.loads(lro_response.text) \n","\n","                display(lro_uri)\n","                #display(lro_response)\n","                display(lro_response.text)\n","                #display(lro_data)\n","\n","                if lro_data[\"status\"] == \"Succeeded\": \n","                    lro_result_response = client.get(lro_result_uri) \n","                    lro_result_data = json.loads(lro_result_response.text) \n","                    print(f\"202:\") \n","                    display(lro_result_data)\n","                    display(lro_result_response.text)\n","                    break \n","                if lro_data[\"status\"]  == \"Failed\": \n","                    print(f\"Failed 202: {lro_data}\") \n","                    break \n","\n","                \n","                print(f\"waiting...\") \n","                time.sleep(retry_delay) \n","        case 500:\n","                print(f\"The sync is already running...\") \n","                display(response.text)\n","        case _:\n","            data = json.loads(response.text) \n","            print(f\"case else:\") \n","            display_return(data)\n","            display(response.text)\n","\n","except Exception as e: print(e)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"844c0656-c847-4935-a3eb-85aec0efc667"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{}},"nbformat":4,"nbformat_minor":5}